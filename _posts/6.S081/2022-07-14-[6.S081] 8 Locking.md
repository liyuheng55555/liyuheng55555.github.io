---
title: 【6.S081】8 Locking
date: 2022-07-14 00:00:08 +/-TTTT
# categories: [TOP_CATEGORIE, SUB_CATEGORIE]
tags: [操作系统, 6.S081]     # TAG names should always be lowercase
author: lyh
---

之前有点写疲了, 学期里面又很忙, 暑假了继续做

---
## xv6 book Chapter6 Locking 知识总结 

操作系统内核通常会让不同进程在不同的CPU上运行，但是使用同一块物理内存。如果两个进程同时访问一个地址，就会产生问题。

xv6内核中到处都是可能被并行访问的数据，比如`kalloc` `kfree`。为了避免发生问题，xv6使用了一系列并行控制技术，**锁**是其中一个。对一个特定的锁，它在任何时刻都只能被一个CPU持有，进而能够保护数据。锁也会影响性能，因为它会把并行操作给串行化。

- **6.1 Race conditions 竞态条件**
  
    举了一个插入链表的例子。如果两个CPU同时向一个链表的头部插入，先插的会被覆盖掉。**竞态条件**指一个内存地址被二者同时访问，并且至少一方是写操作。
- **6.2 Code: Locks**

    xv6中，获取锁的过程大致上是这样
    ```
    21  void
    22  acquire(struct spinlock *lk) // does not work!
    23  {
    24      for(;;) {
    25          if(lk->locked == 0) {
    26              lk->locked = 1;
    27              break;
    28          }
    29      }
    30  }
    ```
    其实这样是不行的，因为第25行也可能被同时访问。多核处理器一般会提供相当于第25、26行功能的原子指令，在RISC-V中是`amoswap r, a`，它把寄存器r和内存地址a的值交换。
- **6.3 Code: Using locks**
  
    任何可能并行访问同一内存地址的时候，都需要锁。但考虑到锁对性能的影响，也不是越多越好。如果不在乎并行，可以搞纯粹的单线程。比如需要把一个针对单核处理器设计的OS放到多核处理器上运行，可以给整个内核上锁。xv6采用了一种有点设计的锁法，每个模块有单独的锁。锁的粒度最终需要由性能测试来决定。
- **6.4 Deadlock and lock ordering**
- **6.5 Locks and interrupt handlers**
- **6.6 Instruction and memory ordering**
  
    很多编译器和CPU都会打乱代码的顺序，在单线程情况下这对结果没有影响，但涉及到多线程和锁就不行了。xv6使用__sync_synchronize()来避免乱序执行。
- **6.7 Sleep locks**
  
    xv6的获取锁操作是忙等待的。如果一个锁被长时间持有，比如磁盘IO可能要持续10ms，这时候忙等就很浪费性能。所以xv6搞了另一种锁，称为sleep-lock，就是在spinlock上面套了一层，让进程获取不到锁的时候进入睡眠状态，等这个锁被释放，进程才被唤醒。
- **6.8 Real World**
  
    有锁编程一直以来都是有挑战性的，很容易忽略一些应当上锁的情况。即使指令集没有实现原子操作，也可以通过软件实现锁，但这样代价很大。如果一个CPU把锁放到了cache里面，而其它CPU更新了锁的状态的话，就很麻烦。为了避免锁导致的问题，可以采用**无锁编程**，但这比有锁编程更难。

- **8.1 Overview**
  
    xv6的文件系统分成七层：
    - 硬盘层：对虚拟硬盘进行读写
    - 缓存层：将硬盘块保存在内存中，并保证每个块只能单线程访问
    - 日志层：记录上层对硬盘的更新，在系统崩溃的时候起到恢复作用
    - inode层：产生文件概念，每个文件为一个inode
    - 目录层：目录是一种特殊的inode
    - 路径名层
    - 文件描述符层：产生管道、设备等概念
- **8.2 Buffer cache layer**
  
    buffer cache对外提供bread和bwrite两个接口。cache能存的block数量有限，如果已经满了，这时需要缓存一个新的块，就得把旧的块换出，这里使用LRU算法。
- **8.3 Code: Buffer cache**
  
    buffer cache是一个双向链表。
    - 首先是数组buf[NBUF]，binit()把buf的所有元素通过链表组织起来
    - buf的valid字段表明是否存了某个block的副本（但是valid=0是装了还是没装？）
    - buf的disk字段没讲清楚，似乎是对buf的修改有没有写回磁盘？
    - bread调用bget
    - bget查看现有的buffer中有没有这个块，没有就尝试换入
    - （存疑）每个扇区只能被缓存一次，使得读取的时候能看到之前的写入
    - bread返回后，调用者就拥有了这块对磁盘的读写权限，写完之后必须bwrite
    - 调用者用完buffer之后，必须brelse
    - brelse把buffer换到list的头部，使得逆向遍历正好符合LRU的顺序

---

## Memory allocator

本来的内存分配器是把所有页面串在一个链表里面，使用一个锁，这样如果多个CPU同时频繁进行内存分配释放的时候，性能就会比较差

一种改善的方法是，给每个CPU设置自己的内存链表，平时从自己这里分配内存，自己的链表空了再去别的链表偷内存

我的策略是每次从空闲内存最多的CPU那里偷一半过来（kmem结构体里面加一个字段统计空闲内存的量）

就是记得释放锁，以及如果出`panic: init exiting`的话，说明init进程无法通过kalloc得到足够的内存，这个进程虽然是以二进制形式加载的，看不到C语言实现，但估计是得不到足够内存就exit()

教程对`#fetch_and_add`和`#acquire`没有完全讲清楚，通过翻代码可以知道

- `#acquire` = `lk->n`，是acquire的次数
- `#fetch_and_add` = `lk->nts`，是忙等待里面那个循环的执行次数
```c
void
acquire(struct spinlock *lk)
{
  push_off(); // disable interrupts to avoid deadlock.
  if(holding(lk))
    panic("acquire");

  __sync_fetch_and_add(&(lk->n), 1);
  while(__sync_lock_test_and_set(&lk->locked, 1) != 0) 
    __sync_fetch_and_add(&(lk->nts), 1);

  __sync_synchronize();
  lk->cpu = mycpu();
}
```
---

## Buffer cache

这个实验和上一个没关系，不做上一个也能做这个

题目说了，让用哈希表，考虑到一共`NBUF=30`个缓存行，那咱的哈希表就设30个哈希桶吧（每个buf一个锁

然后找了个哈希函数，看着挺像那么回事的

```c
unsigned int hash(unsigned int x) {
    x = ((x >> 16) ^ x) * 0x45d9f3b;
    x = ((x >> 16) ^ x) * 0x45d9f3b;
    x = (x >> 16) ^ x;
    return x;
}
```

结果30个哈希桶还真不行，后来看提示说最好质数个哈希桶，那就改成29个哈希桶，然后bcachetest就过了（

但usertests过不了，第一个测试报`panic: freeing free block`，debug了几小时，最后发现是xv6要求每个磁盘块只能进入一个缓存行，因为要维护一致性

在我的设计中，bget如果发现某个块不存在，就一定会给它分配缓存，这里就有**两个进程同时给同一个块分配缓存的可能**

解决方法是用一个锁把分配缓存的过程保护起来，并且在进这个临界区之后要再次检查块是否存在，事实证明进这个临界区的次数很少，不影响bcachetest

感觉不是很优雅，但反正是过了，撒花

