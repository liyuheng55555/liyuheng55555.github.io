[ { "title": "【CS336】BPE分词器训练", "url": "/posts/BPE%E5%88%86%E8%AF%8D%E5%99%A8%E8%AE%AD%E7%BB%83/", "categories": "", "tags": "大模型, CS336", "date": "2025-08-25 00:00:00 +0000", "snippet": "开个新坑～好几年没刷公开课了！实验总结 环境：macbook m3 air 24G 512G TinystoriesV2-GPT4-train数据集：预分词约46秒，merge约5秒 owt_train数据集：预分词约300秒（6并发），merge约1100秒训练可以分为解耦的两大块，预分词、merge预分词使用讲义提供的正则表达式即可。主要优化点就是多进程，这里我电脑只有8核所以就开个6进程，大家的CPU强的话进程数多多易善。owt_train数据集的预分词需要做一下内存控制，在官方提供的pretokenization_example.py中做少许修改就可以实现。由于预分词阶段没啥优化空间，我做了预分词结果缓存来跳过这个步骤，用python的pickle库。预分词的结果我定义为list[tuple[TokenList, Num]]类型，all_token_list即一个[a,p,p,l,e]这样的TokenList到其出现次数的映射，这也是接下来的merge步骤的主数据结构。merge初步实现首先，我实现了一版能通过正确性测试的naive版本： 每次扫描all_token_list中所有的tuple，通过计算得到Pair关系，存入统计表dict[Pair, Num] 然后遍历统计表，得到Num最大的Pair，将其记录到merge_rules列表，这个合并结果记录到词汇表 使用刚生成的这条merge_rule，更新all_token_list 不断循环这个过程，直到词汇表达到上限这里我遇到了一个正确性问题。在更新all_token_list的时候，起初我通过词表对每个TokenList做贪婪最长匹配，这样得到的结果会和标准答案有微小的差别（第一个测试的第21次合并开始才会有差别）。正确的做法是严格按照merge_rule和“合并”的语义来操作。这个bug是claude code写出来的，头疼了一晚上，第二天是gpt5 thinking查出来的。有了这个教训，这个课程我不会再让ai在关键路径上写任何代码……Pair统计缓存有了正确实现之后，要做的优化是，对Pair关系进行缓存，不要每次全量扫all_token_list，毕竟每次合并之后能影响的词只占极少数。这里我首先用了dict[Pair, tuple[Num, set[Index]]这样的结构来做缓存，即Pair关系-&gt;(出现次数, 哪些TokenList有贡献)这个数据结构使得，每轮更新all_token_list的时候只需要处理对上一轮的merge_rule有贡献的那些TokenList，大大减少了计算量。现在，基本就能1.5秒之内通过性能测试了，TinystoriesV2-GPT4-train数据集也能有不错的结果，但owt_train依然会非常慢。主要原因在于，每次找到频率最高的Pair依然需要遍历整个缓存结构，对本课程的数据集来说，缓存结构中会有10w量级的Pair存在。缓存结构优化：有序计数桶这样一来，就需要进一步优化这个缓存结构。为了能快速找到频率最高的Pair，容易想到可以使用堆。但这里有个问题，这个缓存结构是需要不断对已存在的元素进行更新的。堆确实能通过惰性删除等方式来实现更新，不过考虑到这种更新可能堆的尺寸过度膨胀，我选择了另一个方案，有序计数桶。数据结构为一个无序map和一个有序map，分别维护Pair-&gt;数量，和数量-&gt;Pair：class BucketMaxSD: def __init__(self): self.counts: dict[Pair, Num] = {} self.buckets = SortedDict() # Num-&gt;Set(Pair)分析一下时间复杂度： 找到最大的元素：O(1)，由SortedDict保证 更新Pair Num关系：更新counts字典为O(1)，更新SortedDict至多需要做一次删除和一次插入，依然为O(log n)顺便提一句，SortedDict来自第三方库sortedcontainer，其实现原理是一个“分块有序列表”，即list_of_list，而不是常见的红黑树。它通过控制每个子list的长度和两次二分查找来实现写入操作的O(log n)摊还，和最大最小值的O(1)。这个结构在owt_train实测中要好于堆，堆的性能可以参考这位同学：https://zhuanlan.zhihu.com/p/1920487178846344415不过工业训练bpe分词器的过程，往往用的还是堆，参见：https://aclanthology.org/2023.findings-acl.38.pdf?utm_source=chatgpt.com工业bpe的堆同样采用惰性实效的方式进行修改，据说会采用定期重建堆或者只维护top-k候选的方式控制堆的尺寸和内存占用，可能充分优化之后的堆具有更高的性能。另外，工业上很多时候会采用并行merge，训练结果和本课程的单线程merge会有所区别，也有用Unigram这种不同于bpe的算法，有兴趣的同学可以进一步探索。其它优化？我和gpt5做了一些讨论，它给出了一些用双向链表来组织计数桶的方案，但是通过实际数据来看，目前的数据结构在Num较大的尾端是相当稀疏的，元素修改会经常性导致桶的新建和删除，导致双向链表并不适用。那么继续使用有序计数桶，抽样表明merge过程90%的时间都消耗在数据结构的修改上，具体来说删除被merge_rule影响的Pair占据30%时间，新形成的Pair占据70%时间。这里我观察到，在训练的前期，产生每条merge_rule需要数分钟时间，这个时间会越来越低。我想这是因为前期Pair涉及到的贡献者会非常多，例如th组合，显然会有海量的词对这个组合产生贡献，而后期的组合的贡献者会越来越少。这会不会有所启发呢？总体来说，我对课程这个部分已经做得比较满意了，哈哈。部分思考题2.2(a) 相比于 UTF-16 或 UTF-32，在 UTF-8 编码字节上训练我们的分词器有哪些原因更好？比较这些编码对各种输入字符串的输出可能会有帮助。UTF-8的优点在于： 编码较短，训练速度快一些 UTF-8是现在最流行的编码方式，训练出的分词器更泛用 分词器也可以用于ASCII文本，考虑到UTF-8的兼容性(b) 考虑以下（错误的）函数，它旨在将 UTF-8 字节字符串解码为 Unicode 字符串。为什么这个函数是错误的？提供一个产生错误结果的输入字节字符串示例。def decode_utf8_bytes_to_str_wrong(bytestring: bytes): return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])&gt;&gt;&gt; decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))'hello'这个函数没有考虑到UTF-8中存在多个字节表示一个字符的情况，而是将每个字节分别解码了。任何中文字符都能触发问题。2.7问题 (tokenizer_experiments): 分词器实验（4 分）(a) 从 TinyStories 和 OpenWebText 采样 10 个文档。使用你之前训练的 TinyStories 和 OpenWebText 分词器（分别为 10K 和 32K 词汇表大小），将这些采样文档编码为整数 ID。每个分词器的压缩比（字节/标记）是多少？(b) 如果你用 TinyStories 分词器分词你的 OpenWebText 样本会发生什么？比较压缩比和/或定性描述会发生什么。这里我train版本训练的分词器，交叉用valid版本的数据做了测试，压缩比为： – TinyStories训练BPE OWT训练BPE TinyStories数据 4.11 4.00 OWT数据 3.14 4.33 可以发现两个现象： 用数据集自身训练出的分词器，对数据集自身进行分词，压缩比好于用其它数据集训练出的分词器 OWT训练的分词器在TinyStories数据上也有不错的压缩比，而TS训练的分词器在OWT数据上就压缩比差异显著。根本原因是： OWT是从互联网搜集的真实文本库，词汇和表达方式更丰富；TS是GPT生成的，词汇限制在3-4儿童常见词中 可以尝试在两个文本中搜索“fuck”“dick”这样的词，TS完全屏蔽了这些不适合小孩读的内容 OWT文本的丰富性使得其训练产生的分词器具有更强的泛用性 (c) 估算你的分词器的吞吐量（例如，字节/秒）。分词 Pile 数据集（825GB 文本）需要多长时间？目前的吞吐量大约是1-2MB/s，比较慢，后面有需要再做优化吧，一方面是可以多进程，另一方面数据结构也可以优化。(d) 使用你的 TinyStories 和 OpenWebText 分词器，将相应的训练和开发数据集编码为整数标记 ID 序列。我们稍后将使用这个来训练我们的语言模型。我们建议将标记 ID 序列化为数据类型为 uint16 的 NumPy 数组。为什么 uint16 是一个合适的选择？因为词表尺寸通常不止256，通常不超过65536，所以uint16就是一个即满足需求又节约空间的选择。" }, { "title": "【15-445】CMU数据库系统 过程记录", "url": "/posts/15-445-1-%E8%BF%87%E7%A8%8B%E8%AE%B0%E5%BD%95/", "categories": "", "tags": "数据库, 15-445", "date": "2022-10-01 00:00:00 +0000", "snippet": " PROJECT #1 - BUFFER POOLLRU REPLACEMENT POLICY容纳所有目前可被替换的frame id，并在需要替换的时候使用LRU算法，我用了一个list&lt;frame_id_t&gt;。 全局的mutex Pin的时候把frame id从链表中删除 Unpin的时候把frame id放到链表头部 Victim从链表尾部寻找BUFFER POOL MANAGER INSTANCE实现Buffer Pool实例。这里函数比较多，有点不好理解，我们一点点分析，下面用BPMI代指BufferPoolManagerInstance。首先，BPMI::disk_manager_是负责直接执行磁盘读写的；其负责写磁盘的成员函数void WritePage(page_id_t page_id, const char *page_data)具有参数page_id，可见page_id是磁盘页面的编号。在Page类中，也有page_id_这一个成员变量，正如指导书所说，Page类会容纳不同的页面。BPMI::AllocatePage()这个函数值得注意，它会分配了一个page_id给调用者，也就是给调用者分配了一个磁盘页面。其中page_id是跳跃分配的，因为可能同时存在多个BPMI，我们用page_id取模的方式决定某个页面由哪个BPMI负责缓存。与此相对的是BPMI::DeallocatePage()，这个函数在2021课程中还没有实现，没有投入使用。BPMI::page_table_将页编号映射到frame编号，我们在对pages_内容进行增删的时候需要维护这个table。其它代码在读代码过程中，发现有这么三种比mutex“智能”一点的锁： lock_guard：构造时自动加锁，析构时自动解锁，无法手动加解锁。 unique_lock：不仅有自动加解锁功能，也支持手动加解锁，在使用condition_variable::wait()函数前需要先获取这个锁。 scope_lock：类似于lock_guard，但可以同时加多个锁。课程还在mutex的基础上自行实现了一个读写锁，支持写写互斥、读写互斥、读读不互斥，并且是读写公平的。实验中会用到DiskManager的ReadPage和WritePage函数，" }, { "title": "【15-445】CMU数据库系统 环境配置 win10+clion+docker", "url": "/posts/15-445-0-%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/", "categories": "", "tags": "数据库, 15-445", "date": "2022-08-28 00:00:00 +0000", "snippet": "在CS144的最后，好像说要肝6.824来着，但看了看好像不是目前最紧要的事情。于是转头去做6.830，这个课实在是问题比较多，网上攻略也不多，最后环境没配出来…那只好做15-445了，虽然之前义正词严地说拒绝C++…在写CS144的过程中，对vscode的补全的响应速度和精准度不是很满意，于是这次体验一下Clion。配置过程参考了https://zhuanlan.zhihu.com/p/458293882，他是主机ubuntu + Clion + docker。在开始配之前，由于21年的gradescope是完整的5个评测，但有效期只到22年底，22年的评测现在又还没发布，两年的配置有一定差别，需要根据当前的时间做出选择。如果是近期看到这篇文章（22年9-10月），那么推荐拉取21年的代码仓库并按本文配即可，然后在22年内做完实验；如果是更晚看到，推荐拉取最新的代码仓库，Dockerfile和本文有些区别。docker配置课程代码仓库在https://github.com/cmu-db/bustub，在不断更新，按commit记录下载2021年的历史版本，我下的是https://github.com/cmu-db/bustub/tree/1458c6e08f83446c12d99f871ea210df6c9660f5。从https://www.docker.com/下载安装docker desktop，win10的话可能有些wsl和hyper-v的问题（特别如果你同时在用vmware的话），这些问题我在以前解决过一次，相信你也能靠google解决。打开powershell，cd到代码仓库文件夹，这里面包括一个Dockerfile，是课程提供给docker环境的同学用的，我们可以利用这个直接build，但是apt安装clang、cmake的过程会比较慢。我提供一个修改过的Dockerfile，会换成清华源，并额外安装gdb和ssh-server：FROM ubuntu:18.04CMD bash# Install Ubuntu packages.# Please add packages in alphabetical order.ARG DEBIAN_FRONTEND=noninteractive# 不安装这个证书就没法换源RUN apt-get -y updateRUN apt install -y ca-certificates# 备份原文件RUN cp /etc/apt/sources.list sources.list.backup# 换成清华源RUN echo \"deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\" &gt; /etc/apt/sources.listRUN echo \"deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\" &gt;&gt; /etc/apt/sources.listRUN echo \"deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\" &gt;&gt; /etc/apt/sources.listRUN echo \"deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\" &gt;&gt; /etc/apt/sources.listRUN echo \"deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\" &gt;&gt; /etc/apt/sources.listRUN echo \"deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\" &gt;&gt; /etc/apt/sources.listRUN echo \"deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\" &gt;&gt; /etc/apt/sources.listRUN echo \"deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\" &gt;&gt; /etc/apt/sources.listRUN apt-get -y updateRUN apt-get -y install \\ build-essential \\ clang-8 \\ clang-format-8 \\ clang-tidy-8 \\ cmake \\ doxygen \\ git \\ g++-7 \\ pkg-config \\ valgrind \\ zlib1g-dev \\ gdb \\ openssh-serverRUN mkdir /var/run/sshdCMD [\"/usr/sbin/sshd\", \"-D\"]将Dockerfile替换为上面的内容，然后构建镜像，这会消耗几分钟时间。docker build . -t bustub # 构建镜像构建完成后，从镜像启动容器，然后进入容器终端。docker run -d --cap-add sys_ptrace -p 127.0.0.1:2222:22 --name bustub bustub # 从镜像启动容器docker exec -it bustub /bin/bash # 进入容器终端ssh配置编辑/etc/ssh/sshd_config这个文件，在里面新增这两条：PermitRootLogin yesUsePAM no重启ssh。/etc/init.d/ssh restart最后，用passwd修改密码。这样在容器内的准备工作就基本完成了。clion配置打开clion，File -&gt; New -&gt; New Project，选择仓库文件夹，语言标准以课程要求为准，现在是C++17。File -&gt; Settings -&gt; Build,… -&gt; Toolchains，新建一个Remote Host配置，Credentials最右边有个齿轮，点击它然后配ssh，照抄即可，密码填你的。Test Connection一下，能连上就行了。点击确认，稍等一会clion会自动检测出远端的编译器、调试器等，把Remote Host拉到最上面，作为默认配置。CMake这栏似乎默认就行，我是这样的：Deployment这栏，应该也默认就行，需要记一下工作目录在容器中映射到的位置，就是/tmp/tmp.巴拉巴拉。保存配置，使用File -&gt; Reload CMake Project重新加载cmake，现在clion的编译目标应该有很多东西了。如果这时候遇到无法include标准库头文件的问题，可以尝试Tools -&gt; Resync with Remote Host。插曲回到容器终端，处理一点小问题。课程提供了几个python脚本，用来做格式检测之类的，在容器中加上可执行权限：cd /tmp/tmp.巴拉巴拉/bustub/build_supportchmod u+x ./*.py接下来是win特有的问题，由于win神奇的\\r\\n换行机制，与linux换行符不一致，还需要额外安装一个工具，在容器中把py文件用这玩意处理一遍，就ok了。apt install dos2unixcd /tmp/tmp.巴拉巴拉/build_supportdos2unix *.py现在，选择正确的编译目标，点击小锤子就可以编译了。Lab0 效果 编译不通过，可点击超链接跳转到错误行 编译通过 调试 starter test 测试通过 gradescope评测到https://www.gradescope.com/注册一个账号，学校选Carnegie Mellon University，填上课程的邀请码。2021年课程邀请码见https://15445.courses.cs.cmu.edu/fall2021/faq.html，有效期到22年年底。如果你按22年的来配，就用22年的邀请码，不过测试会在作业截止时间之后放出。两个课目前都是能加入的。 对于21年Lab0的评测，讲义https://15445.courses.cs.cmu.edu/fall2021/project0/里面有写，应当上传一个包含p0_starter.h文件的压缩包。实践表明，最好压缩包中不包含任何多余的文件，否则在平台上可能出现编译错误。" }, { "title": "【CS144】2 性能优化 6.1Gbit/s", "url": "/posts/CS144-2-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/", "categories": "", "tags": "计算机网络，CS144", "date": "2022-08-19 00:00:00 +0000", "snippet": "主要参考了这位大佬https://zhuanlan.zhihu.com/p/414279516，优化得远没有他仔细，最后快一些想必是是占了硬件的便宜（我的电脑配置为： VmWare虚拟机 CPU：AMD 5600G，分配2×2个内核 内存：给虚拟机分配4G开始尝试优化前，tcp_benchmark的两个结果均为1.8~1.9Gbit/s，TCP各部分所使用的数据结构与算法为： ByteStream：deque&lt;char&gt; Reassembler：类似list&lt;string&gt;，不合并 Sender：发出去的包使用list&lt;TCPSegment&gt;记录尝试1：ByteStream累加生成string（负面效果）//更改前string result = string(buffer.begin(), end_it);//更改后string result = \"\";for (auto it = buffer.begin(); it != end_it; it++) result += *it;速度下降至0.9Gbit/s，显然通过迭代器生成string比累加要快得多尝试2：ByteStream使用基于vector的循环队列（搁置中）主要是考虑到循环队列的pop操作应该比deque高效，但由于不太会写自定义迭代器，反而导致构建string的速度下降，所以先跳过。尝试3：Sender已发出段改用queue保存（无效果）对于已发出而未被确认的段，我们只关心最老的那个，所以可以改为queue。测试结果没有变化，queue和list的差距在本场景下可以忽略不计。尝试4：Sender已发出段去除已确认部分（无效果）对于被部分确认的段，可以把已确认的部分去掉，降低重发的负担。这个修改会导致sender部分测试无法通过。测试结果没有变化，可能是因为在绝大多数情况下，段都是被整体确认的。尝试5：ByteStream直接使用string（效果爆炸）大概就是buffer直接用string，write用+=，peek用substr，pop用erase。速度直升5~6Gbit/s，达到了原来的3倍左右，原生string性能居然这么好，实在是没想到… 尝试gprofgprof用法可参考https://www.cnblogs.com/kangyupl/p/stanford_cs144_labs.html，由于gprof的时间粒度比较粗，我把数据量×10了。从结果来看，48%的时间都花在了main_loop这个测试主函数中，其它函数耗费时间也没有特别突出的，优化空间已经不大。 这里我退回ByteStream使用deque的版本，发现ByteStream占了90%的时间，确实是瓶颈所在。 最终结果可通过真实测试，基准测试6.1/5.3 Gbps。 " }, { "title": "【CS144】1 过程记录", "url": "/posts/CS144-1-%E8%BF%87%E7%A8%8B%E8%AE%B0%E5%BD%95/", "categories": "", "tags": "计算机网络，CS144", "date": "2022-07-25 00:00:00 +0000", "snippet": "因为课程不希望给出代码，所以就记录一下实现思路吧，每个lab的记录也不会太长Lab0 warm upwebget主要是用connect write read eof这四个函数，可能唯一坑的地方在于Connection: close的时候需要\\r\\n两次。byte_stream功能是比较诡异，同一个进程在同一个对象上进行读写。 (做了lab1发现可能是为了解耦)要求不是很全，有些细节需要看测试才能知道。测试封装得很到位，很容易读，比如:test.execute(InputEnded{false}); // 调用InputEnded()函数，预期结果为falsetest.execute(BufferEmpty{false}); // 调用BufferEmpty()函数，预期结果为false测试结果 Lab1 Reassembler实现对字节流的重组。这个需求的corner case非常多，而且需要自行设计数据结构，思路不对的话写出来的代码又长又乱还没法保证正确性，我推倒重来了两次，不过修改设计的过程挺有意思的。考虑过使用list&lt;deque&gt;这种结构，并且在push过程中进行拼接，后来感觉拼接的逻辑不太好写，deque的拼接性能也不太好，就放弃了。由于不再进行拼接，最终使用的结构类似于list&lt;string&gt;，从测试结果来看还是比较快的。容易搞错的地方有几个地方指导书没说清楚，看了测试才懂: index是字节的序号，不是分组的序号 eof = true的意思是，本次的data后面跟着一个eof，在序号上位于这个eof前面的字节依然可以接收，后面的字节就不接收了 _capacity是ByteStream和Reassembler的总容量，也就是说每次接收字符串之后它俩存的字符总数不超过_capacity实现简述在StreamReassembler中，我添加了这些private元素: 定义结构体Segment: 字符串data data首末字节的序号start_idx与end_idx list&lt;Segment&gt; seg_list，储存没组装好的数据 volume，目前有多少字节没组装好 global_index，下个希望收到的字节编号，初始值0 eof_index，此编号后的字节不再接收，初始值UINT64_MAX我的push_substring()逻辑是这样: 如果eof = true，考虑修改eof_index 去除data位于global_index前面的部分 去除data位于eof_index后面的部分 如果seg_list为空，直接插入5 可能有些部分插在seg_list最前面或最后面，单独处理 向seg_list中间插入 从seg_list头部开始扫描，将组装好的部分移动到_output 从seg_list尾部开始扫描，去除超出_capacity的部分 考虑是否调用_output.end_input()测试结果 Lab2 TCP receiver实现TCP接收功能。不要直接照着正版TCP的状态转换图去写，得照着教材给的LISTEN，SYN_RECV，FIN_RECV三状态转换图来写。感觉教材比较惜字如金。。。说unwrapper(n,isn,checkpoint)的功能是获取和checkpoint相比最为closest的序号，我就理解成了≥checkpoint的最小值，但它其实可以多说一句no matter less or more。还有测试数据里有syn+fin这种组合，这个怎么看都是不合法的包，应该直接抛弃啊，但测试认为正确反应是接收这个包并紧接着终止接收。 前不久知乎上还有人问了这个事，人家linux都是直接抛弃的。syn和fin本身也占字节流序号，这个事很容易忽略，导致ackno()错误。 这样设计的缘由是https://www.zhihu.com/question/24792770。Lab3 TCP sender实现TCP发送功能，支持超时重传和累计确认。一共实现4个函数，分别是: bytes_in_flight(): 目前发送过的所有数据中有多少个字节还没有被确认。 fill_window(): 填满发送窗口。 当然，得考虑到自己有没有那么多数据可发。 syn和fin本身也占字节流序号这个事可能会带来一些麻烦。 ack_received(ackno,window_size): 可以认为是TCP接收到了一个带确认的包，于是把这个包携带的ackno和window_size告知sender。 tick(ms_since_last_tick): TCP连接会定期调用这个方法，以便sender可以知道当前时间，并决定是否重传。 consecutive_retransmissions(): 当前重传次数。 send_empty_segment(): 发送一个不带数据段的包。写代码的时间大约50%都在面向测试编程，细节太多了，还好给看具体测试内容，不然估计改一周也过不了。教材给了6种状态，但我只用了其中3种(CLOSED，SYN_SENT，FIN_SENT)，也不知道到Lab4会不会出问题。。。Lab4 TCP connection本Lab除了模拟测试之外还会在真实网络上测试，而真实网络是没法面向测例编程的。这个Lab的难度确实够高，并且需要你有足够好的调试能力和耐心，不然看着一百多个过不去的测试真是会有点绝望。。。在实现上，我起初是跟着教材的方法写，越写越感觉费劲，所以重写并使用了一套完整版的TCP状态机（见TCP维基词条），独立于sender和receiver中的两套状态机，应该算一个比较清晰的实现了。模拟测试对于模拟测试，我大概遇到了这几个bug： ESTABLISHED状态下，只对长度非0的包做出回应，收到长度为0的包本身就是对方的回应，不要进一步回应。 LISTEN状态下需要无视reset包。 如果两端同时启动连接并发出syn，就会发现自己的syn刚发出去就收到对方的syn，此时进入SYN_RECEIVED状态即可。 一个receiver的bug：如果收到的包超过window许可范围，应只截取范围内的，并且如果存在fin标志，由于fin在逻辑上位于包的末尾，需要优先去除它。断断续续调试了3天，通过了所有模拟测试，聊天抓包也是正常的，但真实测试(名字格式类似t_ucS_1M_32K)几乎全都无法通过。真实测试真实测试的不同之处在于，它是通过sponge/txrx.sh进行的，可以搜索测试名并在cmake文件中找到对应的测试指令。通过阅读txrx.sh，可以知道测试名中每个字母代表什么，例如测试名t_ucS_1M_32K中的各项: u: 基于udp的tcp（i是基于IP的tcp） c: 测试中担任client S: 测试发送功能 1M: 总数据量为1MB 32K: 窗口上限为32KB测试中会使用mktemp创建测试文件，并在测试完成后计算文件的哈希值，哈希值不相同则会报错。可以在txrx.sh的195行左右加这两行，以便知道测试文件的路径:echo \"$TEST_IN_FILE\"echo \"$TEST_OUT_FILE\"有个很神奇的事情是，测试结束的时候测试文件会被自动删除，而且我看不出来txrx.sh里面到底哪条指令删除了文件，所以只好把测试文件复制到其它路径再进行查看。可以自行进行小规模的测试，例如128B数据量+8B窗口:\"${PROJECT_SOURCE_DIR}/txrx.sh\" -ucSd 128 -w 8测试发现传的数据量只有8-10字节，远小于128字节，更多测试表明少传字节数是共性的问题。直觉上来说这是十分严重的问题，但模拟测试测不出来，这就很令人费解了。没什么思路，继续阅读txrx.sh的核心逻辑。在最底层的是REF_PROG和TEST_PROG这两个可执行文件，应该都是TCP实现，一个是用来参考的正确实现，另一个是我们自己的实现。这两个TCP实现都可以发起监听或者发起连接，因此我们有了2×2=4个函数ref_listen ref_connect test_listen test_connect它们都接受两个参数，第一个参数是输出重定向的目标，第二个参数是输入重定向的目标。以下面这个测试为例，这是第一个case，测试发送功能，我们的实现担任客户端: ref_listen \"${TEST_OUT_FILE}\" /dev/null test_connect /dev/null \"${TEST_IN_FILE}\"TEST_IN_FILE和TEST_OUT_FILE都是真实存在的文件，/dev/null是linux下的一个特殊文件，无视一切输入输出。这两行的意思是，我们的TCP从TEST_IN_FILE读入数据，通过下层网络协议发送给参考TCP，它把结果写入TEST_OUT_FILE。 如果两个文件相同，测试就通过了。 对接收功能的测试也很类似。duplex测试稍微有点不同，两端会使用同一个TEST_IN_FILE，分别把结果写入TEST_OUT_FILE和TEST_OUT2_FILE，然后比较三个文件的是否相同。 别忘了TCP是双工的。在明白了测试原理之后，我意识到在单向测试（S或者R）中，一方的输出流很快就会关闭，所以TCP需要如下性质： FIN_WAIT2状态下，我方已不再发送数据，但要继续接收对方发来的数据 CLOSE_WAIT状态下，对方已不会再发送数据，但我方可能还要继续发送继续反复进行128 -w 8测试之后，我发现connection的回应发送不完全正确，sender对ackno和win_size的处理也有问题，导致会出现无法关闭连接的情况，所以重写了fill_window()和ack_received()。这样修改之后，基本就不会出现传输数据量过少的情况了，但还存在结果校验错误和发送数据过多的情况。这时候由于测试还是大面积失败，说实话已经有点山穷水尽了，但直觉告诉我可能是ByteStream或者Reassembler存在问题，所以先尝试把ByteStream从我手动实现的环形队列改成了deque，一下子就能通过大部分真实测试了！给了我很大的鼓舞。这时仅有6-10个左右的测试会Fail或者Timeout。接下来我选择在TCP发生状态转换的时候把新状态打印出来，也就发现了sender的计时重传机制中的一个bug，似乎在outstanding(已发送但没被完全确认的段)为空的时候依然会重传，导致segmentation fault。我不再维护显式的变量timer_running，而是改用outstanding是否为空来判断是否需要重传。现在每次只剩1-3个测试会Timeout了。下一个发现的是可能大量调用tick(0)，就有点离谱。。。不过特判一下就好。在最后，发现SYN_RECEIVED状态下可能收到AF，这时候要直接切换到CLOSE_WAIT，如果只切到ESTABLISHED就会导致永远收不到fin。这种切换其实在wiki状态转换图上并不存在，不过意识到就好办了。这时已经有50%的概率通过全部测试了，剩下的偶发性错误可能是因为reassembler有些极特殊情况没考虑到，就不费劲去调试了。性能也算还行吧。测试结果 整个lab4大约花费了一周的空余时间。Lab5 network interface大概是在链路层接口的基础之上实现arp机制。需要维护三个数据结构,我的实现都是map或者multimap: arp_cache: 记录ip地址和mac地址的对应关系, 并记录这条记录产生的时间。 arp_sent_time: 记录上次针对一个ip地址发出arp广播的时间，以避免5秒之内再次发出广播。 waiting_bg: 记录由于缺少mac地址而不能发出去的ip数据报，以及它们的ip，注意对一个ip可能存在多个数据报。和Lab4相比值得写的不多，代码上可能唯一的坑在于链路层帧的payload要这样写：eth.payload() = dgram.serialize().concatenate();如果用BufferList::append的话就会让结果有微小的差别，而且从提示信息上来看你的结果和期待的结果是完全一致的。还有个问题是，当我t_webget超时，尝试手动./apps/webget cs144.keithw.org /nph-hasher/xyzzy的时候，会报shutdown: Bad file descriptor异常，这个怎么看都不是做实验的人应该关心的。事实也证明这不影响t_webget的测试通过，如果测试超时，很可能是由于网络波动，而和上面那个奇怪的异常无关。Lab6 IP router实现路由查询。本lab并不要求实现路由算法，也就是说不要求我们自己决定往路由表中插入什么，而是直接指定路由表的内容，只让我们根据目标ip和最长匹配原则来决定下一跳。比较简单，注意无符号数的溢出问题，比如在判断ttl的时候使用--ttl &lt;= 0这种写法就会溢出。Lab7 putting it all together在真实网络上，利用课程提供的服务器进行中转，实现两个由我们自行实现的网络栈之间的通信。正常来说需要是两个学生各自用自己的实现来通信，但用同一个实现同时作为客户端和服务端也可以。最终测试 总结断断续续一个月时间，做完了7个lab。要说学到了啥，现在肯定是对tcp协议比较熟，debug的耐心更好了，其它似乎也没啥，你也不能指望一两个本科级别的课程就能让技术水平产生脱胎换骨的变化，对吧？课程代码框架写的挺好，能感觉出来测试封装得特别好，可惜我C++不熟，看不出更多东西。C++真挺坑的，一些设计可以用丑陋来形容，坚定了我以后找工作不用C++的决心。下一门课会是数据库，目前打算优先选择MIT6.830而非CMU15-445，虽然后者口碑可能好一些，但它也用C++，6.830用Java就好多了（" }, { "title": "【6.S081】12 总结", "url": "/posts/6.S081-12-%E5%AE%8C%E7%BB%93%E6%84%9F%E6%83%B3/", "categories": "", "tags": "操作系统, 6.S081", "date": "2022-07-20 00:00:30 +0000", "snippet": "剧透1 Xv6 and Unix utilities: 码量最大, 也是最简单的一个lab2 system calls: 系统调用, 需要读的东西比较多, 难度适中3 page tables: 需要全面理解xv6虚存机制, 并大幅修改, 噩梦难度4 traps: 异常, 难度适中, 比较有趣的一个lab5 lazy page allocation: 内存懒分配, 需要部分理解虚存, 难度适中6 Copy-on-Write Fork: 写时复制的fork, 需要全面理解虚存, 难度高7 Multithreading: 多线程, 需要了解一点xv6线程切换, 难度适中8 locks: 锁, 坑不少, 较难, 但有趣9 file system: 码量小, 不熟悉文件系统的话就有难了10 mmap: 文件内存映射, 结合了虚存与文件系统, 码量略大, 难度仅次于lab311 networking: 码量小, 要读一些东西 受锻炼最大的应该是debug的勇气. 不管遇到多少个bug, 都不要怕, 微笑着面对它 大学里肯定也会开操作系统, 建议上完那个再来肝这个, 从二者的区别能收获不少东西 前两个实验比较简单, 而lab3 pgtbl堪称11个实验中最难的, 当时我硬着头皮研究了4天也没搞定, 差点给劝退了, 后来缓了一周, 跳过lab3继续往后做, 做完lab6之后回过头来又花了2天啃下lab3. 写总结的时候, 去看了一下2021年的安排, 基本和20年一样, 唯独lab3改简单了, 确实更合理一些, 不过本系列还是按照20年的实验来. 再说了就算你做21年的实验, 难道就不想挑战一下吗. gdb是非常强大的工具, qemu对gdb有非常完善的支持, 不品尝就太可惜了. gdb的用处不限于: layout split进行C语言-汇编联合调试 用where指令查看函数调用栈, 查看是什么过程触发了panic 用print指令查看任意局部变量和寄存器的值, 甚至*p这样的表达式也是可行的 用thread指令查看不同cpu核心的运行情况 实验一般会提供很细致的hint, 完全依赖hint一般也能做出来, 不过如果自己能先琢磨一下怎么实现会更好 过不了测试却没有什么思路的时候, 可以考虑直接去看测试程序, 有些细节在题面中没有提到. xv6真是非常注重变量命名的统一. 虽然看题和看书可以靠谷歌翻译, 但咱通过啃英文的过程提升了一点阅读能力…" }, { "title": "【6.S081】11 Network", "url": "/posts/6.S081-11-Network/", "categories": "", "tags": "操作系统, 6.S081", "date": "2022-07-20 00:00:00 +0000", "snippet": "实验说要读网卡手册的一堆内容, 其实读Section 3里面的几段就行, 包括环形队列和cmd的一些标志位定义有个比较神奇的地方是 发送队列初始状态下, 头部和尾部都在0, os向尾部写入, 网卡从头部读取 接收队列初始状态下, 头部在0而尾部在RX_SIZE-1, os从尾部的下一个读取, 网卡向头部写入发送队列的行为比较正常, 接收队列就似乎有点反直觉感觉可以这么理解, 接收队列看似是一个队列, 其实是两个队列共存, os所认知的读队列头部也就是是网卡所认知的写队列尾部在网卡看来它只是不断编辑写队列的尾部, os则是从头部读取, 从这个角度来看, 就和发送队列一致了Network按hint来基本能搞定inte1000_transmit(struct mbuf *m){ push_off(); uint32 index = regs[E1000_TDT]; if ((tx_ring[index].status &amp; E1000_TXD_STAT_DD) == 0) { pop_off(); return -1; } if (tx_mbufs[index] != 0) { mbuffree(tx_mbufs[index]); tx_mbufs[index] = 0; } struct tx_desc *tmp = &amp;tx_ring[index]; tmp-&gt;addr = (uint64)(m-&gt;head); tmp-&gt;length = m-&gt;len; tmp-&gt;cmd = 0; tmp-&gt;cmd |= E1000_TXD_CMD_EOP | E1000_TXD_CMD_RS; // ?? tx_mbufs[index] = m; regs[E1000_TDT] = (regs[E1000_TDT] + 1) % TX_RING_SIZE; pop_off(); return 0;}static voide1000_recv(void){ while (1) { push_off(); uint32 index = (regs[E1000_RDT] + 1) % RX_RING_SIZE; if ((rx_ring[index].status &amp; E1000_RXD_STAT_DD) == 0) { // printf(\"g!\\n\"); pop_off(); return; } rx_mbufs[index]-&gt;len = rx_ring[index].length; net_rx(rx_mbufs[index]); rx_mbufs[index] = mbufalloc(0); rx_ring[index].addr = (uint64)(rx_mbufs[index]-&gt;head); rx_ring[index].status = 0; regs[E1000_RDT] = index; printf(\"recv over\\n\"); recv++; printf(\"recv: %d\\n\", recv); pop_off(); }}" }, { "title": "【6.S081】10 Mmap", "url": "/posts/6.S081-10-mmap/", "categories": "", "tags": "操作系统, 6.S081", "date": "2022-07-18 00:00:10 +0000", "snippet": "本实验实现将磁盘文件映射到内存的功能, 用户可对内存中的文件影响进行读取和修改, 修改的结果在一些情况下需要写回磁盘码量较大, 需要考虑的细节较多, 客观来说可能难度仅次于lab3, 不过一些要点例如懒加载在前面的实验已经出现过defs.h以下是主要的功能实现函数void VMA_free(void *vma); // 释放VMA结构体uint64 mmap_set(uint length, int prot, int flags, int fd); // mmap的设置uint64 mmap_unmap(uint64 addr, int length); // 取消mmap的设置void* mmap_detect(uint64 va); // 检测地址va是否处于mmap后的懒加载状态int mmap_handle(uint64 va, void* vma); // 对懒加载的va进行真正加载void mmap_proc_free(int pid); // 释放一个进程的全部mmap资源, exit时使用void mmap_fork(int old_pid, int new_pid); // 复制一个进程的全部mmap资源, fork时使用VMA结构体定义 位于fs.htypedef struct _VMA { int pid; // -1 if unused uint64 address; // vm start addr uint64 length; int prot; // PROT_READ / PROT_WRITE int flags; // MAP_SHARED / MAP_PRIVATE struct file *f;} VMA;trap.c异常分发的时候加上处理mmap懒分配代码的就行else if ((r_scause() == 13 || r_scause() == 15) &amp;&amp; (vma=mmap_detect(r_stval())) ) { mmap_handle(r_stval(), vma);}vm.c上面7个函数的实现全部位于vm.c VMA_free(): 释放VMA结构体, 用一些奇怪的值填充. 这里可能应该f-&gt;ref - 1一下… void VMA_free(void *_vma) { VMA *vma = (VMA*)_vma; vma-&gt;address = -1; vma-&gt;f = 0; vma-&gt;flags = -1; vma-&gt;length = -1; vma-&gt;pid = -1; vma-&gt;prot = -1; } mmap_set(): 设定mmap, 因为是懒分配, 所以不需要做太多事 找到一个空闲的vma 检查用户对文件的访问权限, 如果文件是以只读形式打开的, 就不能以PROT_WRITE + MAP_SHARED形式执行mmap, 但是可以PROT_WRITE + MAP_PRIVATE或者PROT_READ + MAP_SHARED 设定vma的各种字段, 其中address是起始地址, 这里我是随便选了一块, 虚拟内存空间很大很空旷 uint64 mmap_set(uint length, int prot, int flags, int fd) {// find a free vmaVMA *vma = 0;for (int i=0; i&lt;16; i++) { if (vmas[i].pid == -1) { vma = &amp;vmas[i]; break; }}if (vma == 0) panic(\"sys_mmap\");// file access checkvma-&gt;f = myproc()-&gt;ofile[fd];int writable = (prot &amp; PROT_WRITE) != 0;int private = (flags &amp; MAP_PRIVATE) != 0;if (!private &amp;&amp; writable &amp;&amp; !vma-&gt;f-&gt;writable) return -1;vma-&gt;f-&gt;ref++;// other settingvma-&gt;flags = flags;vma-&gt;prot = prot;vma-&gt;length = length;uint64 tmp = 150+(vma-vmas);vma-&gt;address = tmp &lt;&lt; 30;vma-&gt;pid = myproc()-&gt;pid;return vma-&gt;address;} mmap_unmap(addr, length): 解除mmap 找到地址对应的vma, 参数只提供了addr和length, 这样不太好唯一确定vma, 甚至可能跨多个vma, 不过测试中没有包括这种情况 页表中取消映射 由于addr和length未必是页对齐的, unmap的范围需要仔细斟酌一下 MAP_SHARED情况下, 使用writei()写回数据, 用法见filewrite() 修改vma中的起始地址与长度信息, 如果mmap的范围已经全部释放, 就释放vma uint64 mmap_unmap(uint64 addr, int length) {// find the vmaVMA *vma = 0;for (int i=0; i&lt;16; i++) { if (vmas[i].pid == myproc()-&gt;pid &amp;&amp; !(addr + length &lt;= vmas[i].address || vmas[i].address + vmas[i].length &lt;= addr)) { vma = &amp;vmas[i]; break; }}if (vma == 0) return -1;// do pagetable unmappagetable_t pagetable = myproc()-&gt;pagetable;uint64 start = max(PGROUNDUP(addr), vma-&gt;address);uint64 end = 0;if (addr+length &lt; vma-&gt;address+vma-&gt;length) end = PGROUNDDOWN(addr+length);else end = PGROUNDUP(vma-&gt;address+vma-&gt;length);printf(\"pid=%d, munmap: start=%p, end=%p\\n\",myproc()-&gt;pid, start, end);for (uint64 va=start; va!=end; va+=PGSIZE) { if (walkaddr(pagetable, va) != 0) { // mapped pte_t *pte = walk(pagetable, va, 0); if (vma-&gt;flags == MAP_SHARED &amp;&amp; (*pte &amp; PTE_W) != 0) { begin_op(); ilock(vma-&gt;f-&gt;ip); writei(vma-&gt;f-&gt;ip, 1, va, va - vma-&gt;address, PGSIZE); iunlock(vma-&gt;f-&gt;ip); end_op(); } uvmunmap(pagetable, va, 1, 1); // uvmunmap(pagetable_t pagetable, uint64 va, uint64 npages, int do_free) }}// adjust VMAif (addr &lt;= vma-&gt;address) { // munmap from head vma-&gt;address = addr + length; vma-&gt;length = vma-&gt;address + vma-&gt;length - addr+length;}else if (vma-&gt;address + vma-&gt;length &lt;= addr + length){ // munmap from tail vma-&gt;length = addr - vma-&gt;address;}if (vma-&gt;length &lt; 0) { // munmap all VMA_free(vma);}return 0;} mmap_detect(): 在usertrap()中使用, 检测触发异常的地址是否属于mmap懒分配. 这里需要注意的是, 即使地址在某个vma所记录的范围内, 也未必是懒分配, 可能是读写权限异常.我的思路是, 根据地址寻找pte, 如果pte不存在就说明确实处于懒分配状态. // if va belongs to mmap, and not mapped yet, return its VMA// else return 0void* mmap_detect(uint64 va) { // printf(\"mmap_detect %p: \", va); for (int i=0; i&lt;16; i++) { if ( vmas[i].pid == myproc()-&gt;pid &amp;&amp; vmas[i].address &lt;= va &amp;&amp; va &lt; vmas[i].address+vmas[i].length ) { struct proc *p = myproc(); pte_t *pte = walk(p-&gt;pagetable, va, 0); if (pte != 0 &amp;&amp; (*pte &amp; PTE_V) != 0) { printf(\"pte=%p \",pte); goto fail; } // printf(\"pass\\n\"); return (void*)&amp;vmas[i]; } } fail: // printf(\"fail\\n\"); return c0;} mmap_handle(): 在usertrap()中使用, 对之前懒分配地方进行真分配 使用mappages()进行本页面的映射 根据vma所记录的prot, 决定这一页面的权限 使用readi(), 从磁盘读取数据, 填充这个页面 int mmap_handle(uint64 va, void* _vma) {pagetable_t pagetable = myproc()-&gt;pagetable;uint perm = 0;VMA *vma = (VMA*)_vma;if ((vma-&gt;prot &amp; PROT_READ) != 0) perm |= PTE_R;if ((vma-&gt;prot &amp; PROT_WRITE) != 0) perm |= PTE_W;perm |= PTE_V | PTE_U;void *mem = kalloc();memset(mem, 0, 4096);if (mem==0) panic(\"mmap_handle: kalloc\");if (mappages(pagetable, PGROUNDDOWN(va), PGSIZE, (uint64)mem, perm)!=0) panic(\"mmap_hande: mappages\");// read from fileacquiresleep(&amp;(vma-&gt;f-&gt;ip-&gt;lock));int r = readi( vma-&gt;f-&gt;ip, 1, PGROUNDDOWN(va), PGROUNDDOWN(va) - vma-&gt;address, PGSIZE);releasesleep(&amp;(vma-&gt;f-&gt;ip-&gt;lock));if (r&lt;0) panic(\"mmap_handle\");return 0;} mmap_proc_free(pid): 释放该进程占有的所有mmap资源 void mmap_proc_free(int pid) { for (int i=0; i&lt;16; i++) { if (vmas[i].pid == pid) { mmap_unmap(vmas[i].address, vmas[i].length); } }} mmap_fork(old_pid, new_pid): 将old进程的所有vma复制一份给new进程 void mmap_fork(int old_pid, int new_pid) { for (int i=0; i&lt;16; i++) { if (vmas[i].pid == old_pid) { for (int j=0; j&lt;16; j++) { if (vmas[j].pid == -1) { memmove(&amp;vmas[j], &amp;vmas[i], sizeof(VMA)); vmas[j].pid = new_pid; vmas[j].f-&gt;ref++; break; } } } }} 加系统调用前面已经做过很多次了" }, { "title": "【6.S081】9 FS", "url": "/posts/6.S081-9-fs/", "categories": "", "tags": "操作系统, 6.S081", "date": "2022-07-17 00:00:09 +0000", "snippet": "本实验实现了对大文件和符号链接的支持符号链接又称软连接, 性质类似win10下的快捷方式, 只是保存一下源文件的路径硬链接则不同, 是真正意义上给文件创建了多个路径名, 仅当文件的全部硬链接都被删除的时候, 文件才会被真正删除Large files原来每个文件是12个直接索引块+1个间接索引块, 改成11个直接索引块+1个一级间接索引块+1个二级间接索引块, 使得支持的文件大小从267升级到60000+代码基本上就是照着一级间接索引块的葫芦画瓢, 就不放了记得改MAXFILE, 不然测试会说创建的文件不够大Symbolic links新增符号链接类型的文件, 可以链接到任何路径名, 被链接的文件可以是任意形式, 包括普通文件, 其它符号链接文件, 或者不存在都行每次usertests几乎要5分钟, 特别是writebig测试因为支持了大文件而运行的特别慢有些bug还不稳定出现, 比如usertests能过但是make grade就出panic, 就很头疼sysfile.c主要的修改都在这里, 代码不长, 但是想写对还是不太容易 sys_symlink(): 创建符号链接文件, 用writei()将被链接的路径名写到文件中 ```c uint64 sys_symlink(void) { char target[MAXPATH], path[MAXPATH]; if (argstr(0, target, MAXPATH) &lt; 0 || argstr(1, path, MAXPATH) &lt; 0) return -1; begin_op(); struct inode *ip = create(path, T_SYMLINK, 0, 0); if (ip==0) { end_op(); return -1; } writei(ip, 0, (uint64)target, 0, MAXPATH); iunlockput(ip); end_op(); return 0; } sys_open(): 打开文件, 添加对符号链接文件的支持, 在里面加这么一段 struct inode *new_ip; int count = 0; while (ip-&gt;type == T_SYMLINK &amp;&amp; (omode &amp; O_NOFOLLOW) == 0) { // treat as symlink char link_path[MAXPATH]; readi(ip, 0, (uint64)link_path, 0, MAXPATH); if ((new_ip = namei(link_path)) == 0 || count++ &gt; 10) { iunlockput(ip); end_op(); return -1; } iunlockput(ip); ip = new_ip; ilock(ip); } 其它加系统调用 改各种头文件里的定义 不再赘述测试结果做到凌晨6点才调出来, 放张截图吧" }, { "title": "【CS144】0 斯坦福计算机网络 课程介绍与环境配置", "url": "/posts/CS144-0-%E8%AF%BE%E7%A8%8B%E4%BB%8B%E7%BB%8D%E4%B8%8E%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/", "categories": "", "tags": "计算机网络, CS144", "date": "2022-07-15 00:00:00 +0000", "snippet": "继6.S081完事之后, 觉得该补一补网络相关知识, 就选了这门手撸TCP协议的课程课程有一些很个性的要求, 比如对C++编码风格: 不要用malloc() free() new delete 不要用指针 模板 线程 锁 虚函数 不要用C风格字符串, 要用std::string 不要用C风格类型转换 函数调用中最好通过const Type &amp;传递参数 变量尽量用const修饰 变量的作用域要尽量小, 避免全局变量C++不愧是一门特性十分丰富的语言 (从这门课不仅能学到网络知识, 还能掌握一种C++编程范式, 多是一件美事啊 (环境配置主要参考https://stanford.edu/class/cs144/vm_howto/课程推荐VirtualBox+课程提供的镜像, 不过也给了很多选择, 包括使用任意Linux发行版.我的宿主机是win10, 选择vmware+课程提供的镜像, 过程很简单.一开始也尝试了vmware ubuntu虚拟机下安装virtualbox, 虚拟机套虚拟机, 但这样运行课程镜像的时候会报一个AMD-V相关的错误, 网上有个说法是virtualbox在这种套娃情况下只能运行32位系统, 但课程提供的是64位系统.网上选择wsl或者vmware+ubuntu20.04的也有, 自由度应该挺高.vmware使用ova包的方法, 参考https://zouzhongliang.com/index.php/2019/11/11/vmwarexunijidaoruxunixitongdangfangfaova-ovf/启动系统之后, 是一个字体超小的命令行: 这里可能会遇到连不上网的情况, 用ifconfig看下发现只有127.0.0.1这一个网卡, 解决方法为sudo dhclient ens33, 再ifconfig应该看到另一张网卡, 这个新的ip地址就是从宿主机连接的时候用到的.这样环境配置就完成了. 如果你用自己找的Linux发行版, 可能还得运行一个课程组提供的脚本.最后, 我配了vscode remote." }, { "title": "【6.S081】8 Locking", "url": "/posts/6.S081-8-Locking/", "categories": "", "tags": "操作系统, 6.S081", "date": "2022-07-14 00:00:08 +0000", "snippet": "之前有点写疲了, 学期里面又很忙, 暑假了继续做xv6 book Chapter6 Locking 知识总结操作系统内核通常会让不同进程在不同的CPU上运行，但是使用同一块物理内存。如果两个进程同时访问一个地址，就会产生问题。xv6内核中到处都是可能被并行访问的数据，比如kalloc kfree。为了避免发生问题，xv6使用了一系列并行控制技术，锁是其中一个。对一个特定的锁，它在任何时刻都只能被一个CPU持有，进而能够保护数据。锁也会影响性能，因为它会把并行操作给串行化。 6.1 Race conditions 竞态条件 举了一个插入链表的例子。如果两个CPU同时向一个链表的头部插入，先插的会被覆盖掉。竞态条件指一个内存地址被二者同时访问，并且至少一方是写操作。 6.2 Code: Locks xv6中，获取锁的过程大致上是这样 21 void 22 acquire(struct spinlock *lk) // does not work! 23 { 24 for(;;) { 25 if(lk-&gt;locked == 0) { 26 lk-&gt;locked = 1; 27 break; 28 } 29 } 30 } 其实这样是不行的，因为第25行也可能被同时访问。多核处理器一般会提供相当于第25、26行功能的原子指令，在RISC-V中是amoswap r, a，它把寄存器r和内存地址a的值交换。 6.3 Code: Using locks 任何可能并行访问同一内存地址的时候，都需要锁。但考虑到锁对性能的影响，也不是越多越好。如果不在乎并行，可以搞纯粹的单线程。比如需要把一个针对单核处理器设计的OS放到多核处理器上运行，可以给整个内核上锁。xv6采用了一种有点设计的锁法，每个模块有单独的锁。锁的粒度最终需要由性能测试来决定。 6.4 Deadlock and lock ordering 6.5 Locks and interrupt handlers 6.6 Instruction and memory ordering 很多编译器和CPU都会打乱代码的顺序，在单线程情况下这对结果没有影响，但涉及到多线程和锁就不行了。xv6使用__sync_synchronize()来避免乱序执行。 6.7 Sleep locks xv6的获取锁操作是忙等待的。如果一个锁被长时间持有，比如磁盘IO可能要持续10ms，这时候忙等就很浪费性能。所以xv6搞了另一种锁，称为sleep-lock，就是在spinlock上面套了一层，让进程获取不到锁的时候进入睡眠状态，等这个锁被释放，进程才被唤醒。 6.8 Real World 有锁编程一直以来都是有挑战性的，很容易忽略一些应当上锁的情况。即使指令集没有实现原子操作，也可以通过软件实现锁，但这样代价很大。如果一个CPU把锁放到了cache里面，而其它CPU更新了锁的状态的话，就很麻烦。为了避免锁导致的问题，可以采用无锁编程，但这比有锁编程更难。 8.1 Overview xv6的文件系统分成七层： 硬盘层：对虚拟硬盘进行读写 缓存层：将硬盘块保存在内存中，并保证每个块只能单线程访问 日志层：记录上层对硬盘的更新，在系统崩溃的时候起到恢复作用 inode层：产生文件概念，每个文件为一个inode 目录层：目录是一种特殊的inode 路径名层 文件描述符层：产生管道、设备等概念 8.2 Buffer cache layer buffer cache对外提供bread和bwrite两个接口。cache能存的block数量有限，如果已经满了，这时需要缓存一个新的块，就得把旧的块换出，这里使用LRU算法。 8.3 Code: Buffer cache buffer cache是一个双向链表。 首先是数组buf[NBUF]，binit()把buf的所有元素通过链表组织起来 buf的valid字段表明是否存了某个block的副本（但是valid=0是装了还是没装？） buf的disk字段没讲清楚，似乎是对buf的修改有没有写回磁盘？ bread调用bget bget查看现有的buffer中有没有这个块，没有就尝试换入 （存疑）每个扇区只能被缓存一次，使得读取的时候能看到之前的写入 bread返回后，调用者就拥有了这块对磁盘的读写权限，写完之后必须bwrite 调用者用完buffer之后，必须brelse brelse把buffer换到list的头部，使得逆向遍历正好符合LRU的顺序 Memory allocator本来的内存分配器是把所有页面串在一个链表里面，使用一个锁，这样如果多个CPU同时频繁进行内存分配释放的时候，性能就会比较差一种改善的方法是，给每个CPU设置自己的内存链表，平时从自己这里分配内存，自己的链表空了再去别的链表偷内存我的策略是每次从空闲内存最多的CPU那里偷一半过来（kmem结构体里面加一个字段统计空闲内存的量）就是记得释放锁，以及如果出panic: init exiting的话，说明init进程无法通过kalloc得到足够的内存，这个进程虽然是以二进制形式加载的，看不到C语言实现，但估计是得不到足够内存就exit()教程对#fetch_and_add和#acquire没有完全讲清楚，通过翻代码可以知道 #acquire = lk-&gt;n，是acquire的次数 #fetch_and_add = lk-&gt;nts，是忙等待里面那个循环的执行次数 voidacquire(struct spinlock *lk){push_off(); // disable interrupts to avoid deadlock.if(holding(lk)) panic(\"acquire\");__sync_fetch_and_add(&amp;(lk-&gt;n), 1);while(__sync_lock_test_and_set(&amp;lk-&gt;locked, 1) != 0) __sync_fetch_and_add(&amp;(lk-&gt;nts), 1);__sync_synchronize();lk-&gt;cpu = mycpu();} Buffer cache这个实验和上一个没关系，不做上一个也能做这个题目说了，让用哈希表，考虑到一共NBUF=30个缓存行，那咱的哈希表就设30个哈希桶吧（每个buf一个锁然后找了个哈希函数，看着挺像那么回事的unsigned int hash(unsigned int x) { x = ((x &gt;&gt; 16) ^ x) * 0x45d9f3b; x = ((x &gt;&gt; 16) ^ x) * 0x45d9f3b; x = (x &gt;&gt; 16) ^ x; return x;}结果30个哈希桶还真不行，后来看提示说最好质数个哈希桶，那就改成29个哈希桶，然后bcachetest就过了（但usertests过不了，第一个测试报panic: freeing free block，debug了几小时，最后发现是xv6要求每个磁盘块只能进入一个缓存行，因为要维护一致性在我的设计中，bget如果发现某个块不存在，就一定会给它分配缓存，这里就有两个进程同时给同一个块分配缓存的可能解决方法是用一个锁把分配缓存的过程保护起来，并且在进这个临界区之后要再次检查块是否存在，事实证明进这个临界区的次数很少，不影响bcachetest感觉不是很优雅，但反正是过了，撒花" }, { "title": "Haskell趣学指南 笔记", "url": "/posts/Haskell-%E8%B6%A3%E5%AD%A6%E6%8C%87%E5%8D%97/", "categories": "", "tags": "Haskell", "date": "2022-07-11 00:00:00 +0000", "snippet": "更新中Chapter03: Types and Typeclasses Type：型别，:t可获取表达式的型别 Typeclass：类别，类似于Java中的接口 Eq：可判等类 Ord：可比大小类 Bounded：有上下限类 例如(Num a) =&gt; a -&gt; a -&gt; a，(Num a)表明a是Num类Chapter04: 函数的语法模式匹配对收到的参数，从上到下尝试匹配sayMe :: (Integral a) =&gt; a -&gt; String sayMe 1 = \"One!\" sayMe 2 = \"Two!\" sayMe 3 = \"Three!\" sayMe 4 = \"Four!\" sayMe 5 = \"Five!\" sayMe x = \"Not between 1 and 5\"GuardsbmiTell :: (RealFloat a) =&gt; a -&gt; a -&gt; String bmiTell weight height | bmi &lt;= skinny = \"You're underweight, you emo, you!\" | bmi &lt;= normal = \"You're supposedly normal. Pffft, I bet you're ugly!\" | bmi &lt;= fat = \"You're fat! Lose some weight, fatty!\" | otherwise = \"You're a whale, congratulations!\" where bmi = weight / height ^ 2 skinny = 18.5 normal = 25.0 fat = 30.0 用竖线表示选择，类似else if 最下面可以加where，用于把一些东西预先算出来Chapter06: 递归本节最有趣的是这个快排的例子quicksort :: (Ord a) =&gt; [a] -&gt; [a] quicksort [] = [] quicksort (x:xs) = let smallerSorted = quicksort [a | a &lt;- xs, a &lt;= x] biggerSorted = quicksort [a | a &lt;- xs, a &gt; x] in smallerSorted ++ [x] ++ biggerSorted确实挺短挺优雅的，问题是这不是原地排序，快排的一大优势没了反正优雅就对了，性能差点也不重要Chapter07: 高阶函数柯里化 haskell中所有函数都只接受一个参数 如果有谁看似能接受2个参数，比如f :: a -&gt; a -&gt; a类型，它其实应该描述成f :: a -&gt; (a -&gt; a)工作过程其实是这样 f只接受1个参数a，返回一个函数g :: a -&gt; a g也只接受1个参数a，返回值a -&gt;符号是右结合的 能接受更多参数的函数，同理 似乎这种用多个单参函数构成多参函数的思想，就是柯里化 其实有啥高端的呢，f(x,y)中把x确定，不就变成了g(y)吗这块就是让人能看懂型别声明，给高阶函数做铺垫map与filter map：定义很优雅，比 map :: (a -&gt; b) -&gt; [a] -&gt; [b] map _ [] = [] map f (x:xs) = f x : map f xs filter：也很优雅 filter :: (a -&gt; Bool) -&gt; [a] -&gt; [a] filter _ [] = [] filter f (x:xs) | f x = x : (filter f xs) | otherwise = filter f xs lambda最典型的，\\x y -&gt; x+yfold与scan fold：遍历列表，迭代 fold' :: (t1 -&gt; t2 -&gt; t1) -&gt; t1 -&gt; [t2] -&gt; t1 fold' f a [] = a fold' f a (x:xs) = fold' f (f a x) xs scan：和fold类似，只不过返回迭代量历史值的列表函数调用符 $ 作为分隔符，效果类似小括号，sum (map sqrt [1..130]) = sum $ map sqrt [1..130]函数组合符 .接受两个函数，组合产生一个函数(.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c f . g = \\x -&gt; f (g x)Chapter07: Modules感觉不是需要立刻学的，有需要再来查Chapter08: 构造自己的Types and Typeclasses入门可以不指定字段名，Circle和Rectangle相当于Shape类的构造函数，学名值构造子data Shape = Circle Float Float Float | Rectangle Float Float Float Float deriving (Show)s = Circle 1 2 3Record Syntax也可以指定字段名data Person = Person { firstName :: String, lastName :: String, age :: Int, height :: Float, phoneNumber :: String, flavor :: String} deriving (Show)p = Person {firstName = \"ok\", lastName = \"okok\", age = 10, height = 175, phoneNumber = \"1010101010\", flavor = \"??\"}Type parameters 型别参数似乎就是泛型类…不是很懂Derived instancesdata Person = Person { firstName :: String, lastName :: String, age :: Int} deriving (Show, Eq, Read)只要deriving(Read), 并且每个字段都属于Read的, 居然可以解析字符串形式的Person了, 相当于语言自带完善的序列化和反序列化data Day = Monday | Tuesday | Wednesday | Thursday | Friday | Saturday | Sunday deriving (Eq, Ord, Show, Read, Bounded, Enum)然后说可以Monday == Tuesday之类的, 这值构造子似乎也不完全是函数, 很神秘" }, { "title": "北航操作系统Lab4挑战性任务", "url": "/posts/MOS_Lab4_Challenge/", "categories": "TOP_CATEGORIE, SUB_CATEGORIE", "tags": "操作系统", "date": "2022-07-09 20:00:00 +0000", "snippet": "题目概括 依照POSIX标准，在MOS中实现线程功能 实现4个POSIX标准规定的函数：pthread_create,pthread_exit,pthread_cancel,pthread_join 向用户空间提供接口 线程间实现全地址空间共享 依照POSIX标准，实现信号量功能 实现6个POSIX标准规定的函数：sem_init,sem_destroy,sem_try,sem_wait,sem_trywait,sem_getval 完整支持无名信号量 编写对功能的测试程序，给出测试结果实现情况完成了题目要求，并具有几个特色： 支持一个进程开辟任意多个线程 额外实现了fork功能对线程的适配 信号量等待队列为先进先出线程实现介绍线程功能相关定义 定义TCB结构体，作为线程控制块，结构体内容为 struct TCB { LIST_ENTRY(TCB) tcb_link; // 线程链表域 u_int env_id; // 所属进程编号 struct Trapframe tf; // 保存寄存器堆 u_int thread_status; // 线程状态，有FREE、RUNNABLE、NOT_RUNNABLE三种状态 int joined_on; // join：记录对方的线程号 void **p_retval; // join：自身要接收结果的地址 void *retval; // exit：结果存到这里 }; Env结构体删除条目： struct Trapframe env_tf; // trapframe放到TCB中 u_int env_ipc_receiving;\t\t// ipc接收状态标志 Env结构体增加条目： struct TCB_list tcb_list; // 线程调度链表 struct TCB *now;\t// 当前运行的线程 struct TCB *env_ipc_recving_thread;\t// 处于ipc接收状态的线程 内核中分配1024个TCB的空间，将其用tcb_free_list组织起来，并提供五个操纵TCB的函数： tcb_init：将全部tcb插入tcb_free_list tcb_alloc：分配一个tcb tcb_number：获取tcb序号 tcb_get：根据序号获取tcb tcb_free：释放tcb 新增4个系统调用： sys_pthread_create：创建线程 sys_pthread_exit：结束本线程 sys_pthread_cancel：结束另一个线程 sys_pthread_join：等待至另一个线程结束 线程创建 创建进程的时候，给进程分配首个TCB 之后创建每个线程的时候，每次都再申请一个TCB 每个线程有自己的运行栈，0号线程的栈顶是USTACKTOP 每个运行栈占据1个页面，i号线程的栈顶是USTACKTOP - i×BY2PAGE 线程的其它设置包括pc、status等线程运行调度中维护env-&gt;now，以便得知当前运行的是哪个线程调度 依然使用进程调度链表，带优先级的时间片轮转算法 每个进程内部有线程调度链表，无优先级的时间片轮转算法 对于runnable状态的进程，查看其内部有没有runnable的线程 如果有，就运行之 如果没有，就将此进程放到队尾 join与exit这俩是一对，为方便起见，称进行join的线程为A线程，被join的线程称为B线程，执行exit的也是B线程从先后关系来看，存在两种可能的情况： 如果先join后exit： A线程将接收返回值的地址保存到TCB中，然后进入阻塞 B线程在exit的时候需要把返回值填入A线程的接收地址，并唤醒A线程 如果先exit后join： B线程只需把返回值填入TCB A线程发现B线程已经结束，就不需要阻塞，直接从对方TCB中获得返回值 ipc每个进程中只允许一个线程处于recv等待状态，额外记录等待线程的编号，send过来的时候唤醒这个线程即可fork分析的结果是，只改sys_env_alloc就可以，fork.c的内容完全不用改，cow机制挺完善的复制env的时候也同步复制tcb，微调当前tcb的复制品保证在子进程下次被调度的时候一定会选中复制品线程，而不是其它线程线程结束线程结束时，如果有其它线程在join，就把它们的p_retval中填上retval，并把它们唤醒仅当进程的所有线程都结束后，进程才会结束，这是通过修改libos.c实现的信号量实现介绍起初打算在用户态实现，后来发现这样无法保证原子性，例如sem_wait()如果这样做:int sem_wait(Sem *s) { if (sem_not_valid(s)) return EINVAL; if (s-&gt;val==0) syscall_yield(); // !! 此处必须有原子性，否则信号量可能小于零 !! s-&gt;val--; return 0;}所以还是使用系统调用的方式。 定义环形等待队列Queue，以及相关的函数 结构体定义： struct Queue_ { int list[10]; u_int first; u_int last; \t}; 相关函数： queueInit queuePeek queueEmpt queueFull queuePush queuePop 定义Sem结构体： struct Sem_ { u_int magic; // 魔数 u_int val; // 信号量的值 Queue q; // 环形等待队列，自行实现}; sem_init() sem_destroy()在用户态实现 sem_wait() sem_trywait() sem_post()需要原子性，通过系统调用实现 信号量的等待队列是先进先出的 只要等待队列和线程的阻塞唤醒实现完毕，信号量实现起来就比较简单测试设计了5个测试，测试目的分别是： 线程基本操作：create、exit、join、cancel 信号量基本操作：init、wait、post 哲学家进餐问题：线程与信号量综合，检查不同调度策略的死锁存在性 质数筛：如果开启大量线程（50个以上），还能否正确分配、调度 线程与fork综合：是否实现fork对线程功能的适配1 线程基本操作本测试使用3个线程 主线程创建thread1，然后等待thread1结束，查看它的返回值，然后创建thread2 thread1打印0-9999这10000个数，然后返回10086这个数 thread2是一个死循环测试结果显示： thread1能够正常运行，证明create是有效的 当thread1执行完毕，主线程才会恢复执行，也拿到了正确的返回值，证明join和exit是有效的 仅当对thread2执行cancel操作，进程才能结束，证明cancel是有效的2 信号量基本操作本测试使用3个线程与2个信号量 主线程： 创建出2个线程，thread1和thread2 初始化两个信号量s1=1和s2=0 thread1是死循环，内容为 先wait(s1) 打印5行”thread111111” post(s2) thread2也是死循环，内容为 先wait(s2) 打印3行”thread2” post(s1) 预期效果为先打印5行”thread111111”，然后交替打印3行”thread2”与5行”thread111111”，测试结果符合预期3 哲学家进餐本测试使用6个线程与5个信号量这是一个经典的PV操作问题： 一张圆桌上坐着5名哲学家，每两个哲学家之间的桌上摆一根筷子，桌子的中间是一碗米饭。哲学家们倾注毕生的精力用于思考和进餐，哲学家在思考时，并不影响他人。只有当哲学家饥饿时，才试图拿起左、右两根筷子（一根一根地拿起）。如果筷子已在他人手上，则需等待。饥饿的哲学家只有同时拿起两根筷子才可以开始进餐，当进餐完毕后，放下筷子继续思考。如果每个哲学家都选择先拿左手的筷子，后拿右手的筷子，就可能每个哲学家都只拿起了左手的筷子，导致死锁此问题有一个避免死锁的策略，就是偶数编号的哲学家先拿左手边的筷子，奇数编号的哲学家先拿右手边的筷子3号测试模拟了这个问题，并尝试了上述的两种策略为了更容易触发死锁，在问题中加一个设定，就是在哲学家拿起第一根筷子后，会思考一会，再尝试拿第二根筷子；另外设定每个哲学家都只会进餐100次，之后线程就结束实际测试中，两种调度策略的效果都符合预期4 质数筛哇哈哈，从6.S081抄来的创意，从多进程改成了多线程本测试使用50+个线程，100+个信号量 主线程： 创建1号线程，向其发送2-200的所有数 i号线程： 收到的第一个数，认为是质数，打印出来 接下来收到的所有数，用第一个数去除，如果不能整除，就发送到i+1号线程 测试结果表明，单个进程开100以下的线程一般不会出问题5 线程与fork综合测试就很简单地测了一下，主线程先pthread_create出一个新线程，再fork，新线程也确实执行了两次，说明问题不大，但在多次fork的时候存在问题，最后没时间了就没再管这块" }, { "title": "【6.S081】3 Pagetable", "url": "/posts/6.S081-3-pgtbl/", "categories": "", "tags": "操作系统, 6.S081", "date": "2022-03-15 00:00:03 +0000", "snippet": "全课程最难的实验，没有之一，坑不计其数建议先跳过，等做完懒分配、cow fork之后再来做Print a page table (easy)这个题我没按要求写另外建议后面单独写一个打印进程内核页表中用户部分的vmprint，不然打印结果会多出很多内核映射voidvmprint(pagetable_t pagetable, int level){ if (level==2) { printf(\"-------- pagetable %p --------\\n\", pagetable); } // there are 2^9 = 512 PTEs in a page table. for(int i = 0; i &lt; 512; i++){ pte_t pte = pagetable[i]; if(pte &amp; PTE_V){ if (level&lt;2) printf(\".. \"); if (level&lt;1) printf(\".. \"); printf(\"..%d: \", i); uint64 pa = PTE2PA(pte); printf(\"pte %p\\n\", pte); if (level&gt;0) vmprint((pagetable_t)pa, level-1); } } if (level==2) { printf(\"-----------------------------------------------\\n\"); }}A kernel page table per process (hard)在xv6中，内核态和用户态使用两张不同的页表，从而保证了用户态不能随意访问内核中的信息。这也带来了一些问题，比如read系统调用，用户程序会要求内核把信息写到特定的地址，但内核没法直接用这个地址，因为这是个用户态虚拟地址，没法通过内核页表来翻译。在原版的xv6中，解决方法是通过软件计算出用户虚拟地址对应的物理地址，然后内核将信息写入到该物理地址，曲线救国。这样做的坏处在于，软件转换地址比较慢。本实验要实现的效果是，不再使用统一的内核页表（除非目前没有进程在运行），而是使用每个进程自己的进程内核页表，这个页表中既包含了内核地址映射又包含了用户地址映射，从而内核可以直接使用用户虚拟地址，不再需要软件转换。在另一个操作系统MOS中，不存在这个问题，因为不存在内核页表这种东西，属于内核的地址从硬件上就映射到了低地址，所以在类似read系统调用的时候内核可以继续使用用户页表defines.h// 后缀为1的和原版基本一致，唯一区别是对kpg而非全局内核页表执行操作uint64 kvmpa1(pagetable_t kpg, uint64 va);void kvmmap1(pagetable_t kpg, uint64 va, uint64 pa, uint64 sz, int perm);// kpg专属操作pagetable_t new_kpg(); // 创建kpgvoid free_kpg(pagetable_t kpg); // 递归释放kpg本身占据的空间void flush_kpg(struct proc *p, int type); // 用户页表更新时，更新kpgvoid switch_pagetable(pagetable_t pg); // satp切换到指定页表，类似kvminithart()void vmprint(pagetable_t, int);proc.hstruct proc { ...... uint64 kpgsz; // kpg包含用户内容的size，sbrk的时候有用 pagetable_t kpg; // 进程内核页表的根地址 ......}vm.c kvmpa1基本和原版一样 kvmmap1基本和原版一样 switch_pagetable和kvminithart基本一样 new_kpg和kvminit差不多 free_kpg和freewalk差不多，只是freewalk要求叶子页面必须已经释放，free_kpg则不要求叶子已释放，只把页表本身占据的空间释放 // 只释放页表本身占据的空间，不释放映射的那些物理内存void free_kpg(pagetable_t kpg) { // there are 2^9 = 512 PTEs in a page table. for(int i = 0; i &lt; 512; i++){ pte_t pte = kpg[i]; if((pte &amp; PTE_V) &amp;&amp; (pte &amp; (PTE_R|PTE_W|PTE_X)) == 0){ // this PTE points to a lower-level page table. uint64 child = PTE2PA(pte); free_kpg((pagetable_t)child); } kpg[i] = 0; } kfree((void*)kpg);} uvmcopy_onlypagetable，一个flush_kpg专用的函数，从uvmcopy修改得来，只复制页表不复制物理内存，后面cow fork实验也能用到这个函数 intuvmcopy_onlypagetable(pagetable_t old, pagetable_t new, uint64 va_start, uint64 va_end){ pte_t *pte; uint64 pa, i; uint flags; // char *mem; for(i = PGROUNDDOWN(va_start); i &lt; va_end; i += PGSIZE){ if((pte = walk(old, i, 0)) == 0) panic(\"uvmcopy: pte should exist\"); if((*pte &amp; PTE_V) == 0) panic(\"uvmcopy: page not present\"); pa = PTE2PA(*pte); flags = PTE_FLAGS(*pte); if(mappages(new, i, PGSIZE, (uint64)pa, flags) != 0){ goto err; } } return 0;err: uvmunmap(new, 0, i / PGSIZE, 1); return -1;} flush_kpg是这个实验的重点，kpg的调整全靠这个函数。 exec、fork、sbrk的时候都需要调整内核页表，分别处理，userinit和fork遇到的情况是一样的。 exec：将一个进程替换成另一个进程，kpg不为空，需要先把kpg的用户部分清空再复制过去一份完整的用户页表 fork/userinit：创建新进程，新进程的kpg为空，直接复制用户页表就行 sbrk：调整进程占据的内存空间，又分缩小和增大两种情况 缩小：kpg中unmap对应的部分 增大：从用户页表向kpg中copy 这里需要面对PGROUNDUP/PGROUNDDONE、trapframe等一系列细节问题，很容易导致各种panic。 除非有充分的理由，不要删除任何panic，这样只是延后了问题爆发的时间，增大了问题溯源的难度。 这里有个很简单的做法，就是每次用户页表更改之后，都把kpg的用户部分清空然后重新copy进去。 这样测试会超时，因为测试中有很多sbrk。 void flush_kpg(struct proc *p, int type) { // pte_t *pte; // printf(\"process %s flush_kpg type %d\\n\", p-&gt;name, type); if (type==0) { // exec // 更新trapframe uvmunmap(p-&gt;kpg, TRAPFRAME, 1, 0); uvmunmap(p-&gt;kpg, 0, PGROUNDUP(p-&gt;kpgsz)/PGSIZE, 0); // 进程内核页表的用户部分全部清除 mappages(p-&gt;kpg, TRAPFRAME, PGSIZE, (uint64)(p-&gt;trapframe), PTE_R|PTE_W); uvmcopy_onlypagetable(p-&gt;pagetable, p-&gt;kpg, 0, p-&gt;sz); for(int va=0; va&lt;p-&gt;sz; va+=PGSIZE) uvmclear(p-&gt;kpg, va); } else if (type==1) { // fork // 进程内核页表的用户部分本身就为空，不需要清除 mappages(p-&gt;kpg, TRAPFRAME, PGSIZE, (uint64)(p-&gt;trapframe), PTE_R|PTE_W); uvmcopy_onlypagetable(p-&gt;pagetable, p-&gt;kpg, 0, p-&gt;sz); for(int va=0; va&lt;p-&gt;sz; va+=PGSIZE) uvmclear(p-&gt;kpg, va); } else if (type==2) { if (PGROUNDDOWN(p-&gt;sz) != PGROUNDDOWN(p-&gt;kpgsz)) { // sbrk // 分为空间增大和缩小两种 if (p-&gt;sz &lt; p-&gt;kpgsz) { // 缩小 for (int va=PGROUNDUP(p-&gt;sz); va&lt;p-&gt;kpgsz; va+=PGSIZE) uvmunmap(p-&gt;kpg, va, 1, 0); } else if (p-&gt;sz &gt; p-&gt;kpgsz) { // 增大 uvmcopy_onlypagetable(p-&gt;pagetable, p-&gt;kpg, PGROUNDUP(p-&gt;kpgsz), p-&gt;sz); for(int va=PGROUNDDOWN(p-&gt;kpgsz); va&lt;p-&gt;sz; va+=PGSIZE) uvmclear(p-&gt;kpg, va); } else printf(\"flush_kpg type 2 p-&gt;sz == p-&gt;kpgsz\\n\"); } } else { panic(\"flush_kpg gg\\n\"); } p-&gt;kpgsz = p-&gt;sz; return ;} 修改两个copy，新的copy内部完全没用到页表，传0即可 intcopyin(pagetable_t pagetable, char *dst, uint64 srcva, uint64 len){ return copyin_new(0, dst, srcva, len);}intcopyinstr(pagetable_t pagetable, char *dst, uint64 srcva, uint64 max){ return copyinstr_new(0, dst, srcva, max);} proc.c把创建内核栈的代码，从procinit挪到allocproc同时，freeproc的时候需要释放内核栈占据的物理页面这个地方hint完全没提，而且本来内核栈是不需要释放的，这是我遇到的最后一个坑点总结还有一些零碎的坑点，就不一一列举了（因为我想不起来了）== Test count copyin == $ make qemu-gdbcount copyin: OK (0.4s) == Test usertests == $ make qemu-gdb(153.7s) == Test usertests: copyin == usertests: copyin: OK == Test usertests: copyinstr1 == usertests: copyinstr1: OK == Test usertests: copyinstr2 == usertests: copyinstr2: OK == Test usertests: copyinstr3 == usertests: copyinstr3: OK == Test usertests: sbrkmuch == usertests: sbrkmuch: OK == Test usertests: all tests == usertests: all tests: OK == Test time == time: OK " }, { "title": "【6.S081】7 Thread", "url": "/posts/6.S081-7-thread/", "categories": "", "tags": "操作系统, 6.S081", "date": "2022-02-28 00:00:07 +0000", "snippet": "xv6在线程上有一些神奇之处, 本文首先试图搞清楚第一个用户进程是如何启动的 第一个用户进程的本体位于proc.c中的initcode数组, 以十六进制数组的形式存在. 在main()函数初始化的结尾调用了userinit()将init进程设置好, 这个设置过程调用了allocproc()和uvminit(). allocproc()分配进程控制块, 需要注意的是进程的context.ra被设为函数forkset(). context.ra将是此进程首次调度去到的地方. uvminit()为进程分配一页内存, 然后将initcode数组复制到这块内存 此时init进程已经就绪, 等待被调度 从userinit()返回到main()函数后, 跳过中间的部分, 直接来到最后的schedulor() schedulor()的主体是个无限循环, 每次循环都可能选中一个进程来运行, 单次循环的过程用下图描述 其它用户进程的启动也与此类似. 如果不是首次运行, 那就从scheduler()切换到yield()而非forkret(), 其它部分完全相同 这里似乎强调了在swtch过程中cpu必须持有该进程的锁, 在我看来有另一种解决方法, 就是把进程状态设为SWTCHING状态, 此状态下其它cpu不会尝试调度这个进程. 不过修改了之后, 启动过程中会发生死锁, 看来还是有理解不到位的地方.Uthread: switching between threads本实验要求实现用户级线程, 用自行实现的yield切来换线程, 而不是靠时钟中断驱动.可以看出, 时钟中断是非自愿调度, 而本实验是自愿式调度.给线程控制块增加记录上下文的字段.struct thread { char stack[STACK_SIZE]; /* the thread's stack */ int state; /* FREE, RUNNING, RUNNABLE */ uint64 regs[20];};上下文的初始值设置, 只需要设置ra和sp即可. t-&gt;regs[0] = (uint64)func; // ra设在函数开头 t-&gt;regs[1] = (uint64)(t-&gt;stack+STACK_SIZE); // sp设在栈的最高处，栈是从高地址向低地址增长的uthread_switch.S 照抄 kernel/swtch.S，功能完全一样Using threads 为什么两个进程的时候会出现键丢失，而一个进程的时候不会？ 举一个会导致键丢失的例子。两个进程，其中一个进程插keys[0…49999]，另一个进程插keys[50000…99999].如果两个进程同时插入同一个bucket，新创建的两个struct entry中，略早一些插入的确实会成为链表头部，但很快就会被覆盖，略晚一些的才会成为真正的链表头部，早一些的那个就丢失了.不加锁2个线程时出现了键丢失的情况$ ./ph 1100000 puts, 7.710 seconds, 12969 puts/second0: 0 keys missing100000 gets, 7.676 seconds, 13027 gets/second$ ./ph 2100000 puts, 3.460 seconds, 28901 puts/second1: 16444 keys missing0: 16444 keys missing200000 gets, 8.384 seconds, 23855 gets/second加一个全局锁单线程和之前没啥区别，猜测是单线程的时候系统会无视加解锁操作.双线程的插入反而比单线程慢，这个好理解，全局锁导致插入完全单线程化，并且加解锁也需要时间.static void put(int key, int value){ int i = key % NBUCKET; // is the key already present? struct entry *e = 0; for (e = table[i]; e != 0; e = e-&gt;next) { if (e-&gt;key == key) break; } pthread_mutex_lock(&amp;lock[i]); // 加锁 if(e){ // update the existing key. e-&gt;value = value; } else { // the new is new. insert(key, value, &amp;table[i], table[i]); } pthread_mutex_unlock(&amp;lock[i]); // 解锁}./ph 2100000 puts, 3.696 seconds, 27054 puts/second0: 0 keys missing1: 0 keys missing200000 gets, 7.378 seconds, 27108 gets/second给每个bucket加锁没有键丢失, 且插入达到了两倍速度static void put(int key, int value){ int i = key % NBUCKET; // is the key already present? struct entry *e = 0; for (e = table[i]; e != 0; e = e-&gt;next) { if (e-&gt;key == key) break; } pthread_mutex_lock(&amp;lock[i]); // 加锁 if(e){ // update the existing key. e-&gt;value = value; } else { // the new is new. insert(key, value, &amp;table[i], table[i]); } pthread_mutex_unlock(&amp;lock[i]); // 解锁}./ph 2100000 puts, 3.696 seconds, 27054 puts/second0: 0 keys missing1: 0 keys missing200000 gets, 7.378 seconds, 27108 gets/secondBarrierstatic void barrier(){ // YOUR CODE HERE // // Block until all threads have called barrier() and // then increment bstate.round. // pthread_mutex_lock(&amp;(bstate.barrier_mutex)); bstate.nthread++; if (bstate.nthread==nthread) { // 相等意味着本线程是最后一个调用barrier的 bstate.nthread = 0; bstate.round++; pthread_cond_broadcast(&amp;(bstate.barrier_cond)); } else { pthread_cond_wait(&amp;(bstate.barrier_cond), &amp;(bstate.barrier_mutex)); } pthread_mutex_unlock(&amp;(bstate.barrier_mutex));}" }, { "title": "【6.S081】6 COW fork", "url": "/posts/6.S081-6-COW-fork-copy/", "categories": "", "tags": "操作系统, 6.S081", "date": "2022-02-25 00:00:06 +0000", "snippet": "cow fork = 牛叉这是做完实验4个月之后整理的（感谢git告诉我当时改了哪里），一些细节和坑点已经想不起来了Implement copy-on writefork的时候，不进行物理页面的拷贝，让父子进程共享同样的物理页面，等到有一方要向页面写入的时候再拷贝页面新增函数：// kalloc.cvoid* kcopy(const void *pa); // 复制pa所在的物理页面void krefadd(const void *pa); // 给pa所在的物理页面引用数+1// vm.cvoid vmprint(pagetable_t pagetable, int level);// 类似uvmcopy，但只复制页表不复制物理页面int uvm_samemap(pagetable_t old, pagetable_t new, uint64 sz);// 检查va页面是否处于cow状态int uvm_cow_detect(pagetable_t pagetable, uint64 va);// 将va页面所对应物理页面复制一份，并重新映射到复制品int uvm_change(pagetable_t pagetable, uint64 va);kalloc.c 新建一个统计物理页面引用数的全局数组，并在kinit()中初始化： struct spinlock ref_lock; // 只考虑用户能用到的物理内存，KERNBASE以下都归内核所有 int ppage_refcnt[(PHYSTOP-KERNBASE)/PGSIZE+1000]; void kinit() { initlock(&amp;kmem.lock, \"kmem\"); initlock(&amp;ref_lock, \"ref\"); acquire(&amp;ref_lock); for (int i=0; i&lt;(PHYSTOP-KERNBASE)/PGSIZE+1000; i++) ppage_refcnt[i] = 0; release(&amp;ref_lock); freerange(end, (void*)PHYSTOP); } kalloc()的时候给引用数加一 kfree()的时候给引用数减一，如果引用数为0才会真的释放物理页面 void kfree(void *pa) { struct run *r; if(((uint64)pa % PGSIZE) != 0 || (char*)pa &lt; end || (uint64)pa &gt;= PHYSTOP) panic(\"kfree\"); int idx = PA2REF((uint64)pa); acquire(&amp;kmem.lock); acquire(&amp;ref_lock); if (ppage_refcnt[idx]&gt;0) { ppage_refcnt[idx]--; } if (ppage_refcnt[idx]==0) { // Fill with junk to catch dangling refs. memset(pa, 1, PGSIZE); r = (struct run*)pa; r-&gt;next = kmem.freelist; kmem.freelist = r; } release(&amp;kmem.lock); release(&amp;ref_lock); } krefadd()给引用数+1 void krefadd(const void *pa) { if(((uint64)pa % PGSIZE) != 0 || (char*)pa &lt; end || (uint64)pa &gt;= PHYSTOP) panic(\"kref\"); int idx = PA2REF((uint64)pa); acquire(&amp;ref_lock); ppage_refcnt[idx]++; release(&amp;ref_lock); } kcopy()复制产生一个新的物理页面 ```c void* kcopy(const void pa) { if(((uint64)pa % PGSIZE) != 0 || (char)pa &lt; end || (uint64)pa &gt;= PHYSTOP) panic(“kcopy”); char *mem = kalloc(); if (mem==0) return 0; memmove(mem, pa, PGSIZE); return mem; }`vm.c uvm_samemap(old_table,new_table,size)类似uvmcopy，但只复制页表，新旧页表映射到相同的物理页面，同时把页表项COW位设为1，也就是让页面进入cow状态 // copy only pagetable int uvm_samemap(pagetable_t old, pagetable_t new, uint64 sz) { pte_t *pte; uint64 pa, i; uint flags; // char *mem; for(i = 0; i &lt; sz; i += PGSIZE){ if((pte = walk(old, i, 0)) == 0) panic(\"uvmcopy: pte should exist\"); if((*pte &amp; PTE_V) == 0) panic(\"uvmcopy: page not present\"); pa = PTE2PA(*pte); flags = PTE_FLAGS(*pte); flags |= PTE_COW; flags &amp;= ~PTE_W; if(mappages(new, i, PGSIZE, pa, flags) != 0){ goto err; } uvmsetflags(pte, flags); krefadd((const void*)pa); } return 0; err: uvmunmap(new, 0, i / PGSIZE, 1); return -1; } uvm_change(pagetable,va)将va映射到一个新的物理页面，也就是解除页面的cow状态 int uvm_change(pagetable_t pagetable, uint64 va) { pte_t *pte = walk(pagetable, va, 0); uint64 pa = PTE2PA(*pte); void *newpa = kcopy((void*)pa); if (newpa==0) return -1; int flags = PTE_FLAGS(*pte); flags &amp;= ~PTE_COW; flags |= PTE_W; uvmunmap(pagetable, PGROUNDDOWN(va), 1, 1); if (mappages(pagetable, PGROUNDDOWN(va), PGSIZE, (uint64)newpa, flags)==-1) return -1; return 0; } uvm_cow_detect(pagetable,va)检测va是否处于cow状态 int uvm_cow_detect(pagetable_t pagetable, uint64 va) { pte_t *pte = walk(pagetable, va, 0); return pte!=0 &amp;&amp; (*pte &amp; PTE_V) != 0 &amp;&amp; (*pte &amp; PTE_U) != 0 &amp;&amp; (*pte &amp; PTE_W) == 0 &amp;&amp; (*pte &amp; PTE_COW) != 0 ; } trap.c usertrap()增加对cow的检测和处理 if(r_scause() == 8){ ... } else if((which_dev = devintr()) != 0){ ... } else if(r_scause()==15) { pagetable_t pagetable = myproc()-&gt;pagetable; uint64 va = r_stval(); if (uvm_cow_detect(pagetable, va)) { pte_t *pte = walk(pagetable, va, 0); int idx = PTE2REF(*pte); if (ppage_refcnt[idx]&gt;1) { if (uvm_change(pagetable, va)==-1) { printf(\"change fail\\n\"); p-&gt;killed = 1; } } else { *pte |= PTE_W; *pte &amp;= ~PTE_COW; } } else { printf(\"scause==15 but not cow\\n\"); p-&gt;killed = 1; } } else { ... } proc.c fork()中使用uvm_samemap()代替uvmcopy()" }, { "title": "【6.S081】5 Lazy allocation", "url": "/posts/6.S081-5-lazy-allocation/", "categories": "", "tags": "操作系统, 6.S081", "date": "2022-02-18 00:00:05 +0000", "snippet": "xv6 book 知识点总结4.1 risc-v trap 机制risc-v cpu有一系列和trap相关的寄存器，包括： sepc：trap发生的时候，pc存到这里，以备trap处理完返回 stvec：trap处理程序的地址 scause：描述trap的起因（缺页、计时器中断等等） sscratch：暂时不明 sstatus：控制trap使能之类的，比如其中的SIE位控制外部中断的使能 stval：虚存地址解析失败的时候，这里会保存下那个虚存地址，也许有其他作用这些寄存器在用户模式下无法读写，在特权模式下可读可写（但不是随便写）risc-v cpu处理trap的一般流程是： 如果是外部中断，就检查sstatus的SIE位，如果为0就不理睬这个中断 开始处理trap，先把SIE置0，不允许新的中断到来 $epc &lt;- $pc 把“当前是用户模式还是特权模式”保存在sstatus的SPP位 根据trap类型，设置scause 变更到特权模式 $pc &lt;- $stvec 执行trap处理程序根据risc-v的设计思想，硬件上处理trap只做最少的、必做的事，其它事情例如切换satp、保存寄存器都交给操作系统，这是为了给操作系统设计提供最大的自由度4.6 页错误xv6中有三种页错误： load页错误 store页错误 取指页错误xv6目前处理页错误的方式很简单，进程导致页错误就把该进程杀掉，kernel导致页错误就进入panic。实际上可以做出更灵活的响应。Eliminate allocation from sbrk()删除sbrk的实际内存分配，只把进程控制块中记录的size增大懒分配实现修改trap.c中的usertrap，使其额外检查此异常是不是懒分配导致的，如果是的话就给触发异常的地址实际分配内存这里需要在vm.c里面加两个函数// 检测地址va是否属于懒分配int uvm_lazy_detect(uint64 va) { struct proc *p = myproc(); pte_t *pte; return va&lt;p-&gt;sz &amp;&amp; PGROUNDDOWN(va) != r_sp() &amp;&amp; (((pte = walk(p-&gt;pagetable, va, 0))==0) || ((*pte &amp; PTE_V)==0));}// 给va所在的页面分配内存void uvm_lazy_alloc(uint64 va) { struct proc* p = myproc(); char *mem = kalloc(); if (mem==0) { // printf(\"gg\"); p-&gt;killed = 1; } else{ memset(mem, 0, PGSIZE); if (mappages(p-&gt;pagetable, PGROUNDDOWN(va), PGSIZE, (uint64)mem, PTE_W|PTE_R|PTE_X|PTE_U)!=0) { kfree(mem); p-&gt;killed = 1; } }}使用这两个函数，完善usertrap()if(r_scause() == 8){ ...} else if((which_dev = devintr()) != 0){ ...} else if ((r_scause()==13 || r_scause()==15) &amp;&amp; uvm_lazy_detect(r_stval())) { uvm_lazy_alloc(r_stval()); // 懒分配} else { ...}（riscv指令参考书有云，scause为13的时候是Load Page Fault，15是Store Page Fault）测试echo进程的初始状态，没有经历过懒分配，可以看到已经分配了虚存最低位的4个页面，以及最高位的trampoline和trapframe页面..0 pte = 0x0000000021fdc801 pa = 0x0000000087f72000.. ..0 pte = 0x0000000021fd9401 pa = 0x0000000087f65000.. .. ..0 pte = 0x0000000021fdc05f pa = 0x0000000087f70000.. .. ..1 pte = 0x0000000021fd98df pa = 0x0000000087f66000.. .. ..2 pte = 0x0000000021fdc40f pa = 0x0000000087f71000.. .. ..3 pte = 0x0000000021fd68df pa = 0x0000000087f5a000..255 pte = 0x0000000021fdd001 pa = 0x0000000087f74000.. ..511 pte = 0x0000000021fdcc01 pa = 0x0000000087f73000.. .. ..510 pte = 0x0000000021fd90c7 pa = 0x0000000087f64000.. .. ..511 pte = 0x0000000020001c4b pa = 0x0000000080007000第一次懒分配4号虚存页面lazy allocationvmprint:..0 pte = 0x0000000021fdc801 pa = 0x0000000087f72000.. ..0 pte = 0x0000000021fd9401 pa = 0x0000000087f65000.. .. ..0 pte = 0x0000000021fdc05f pa = 0x0000000087f70000.. .. ..1 pte = 0x0000000021fd98df pa = 0x0000000087f66000.. .. ..2 pte = 0x0000000021fdc40f pa = 0x0000000087f71000.. .. ..3 pte = 0x0000000021fd68df pa = 0x0000000087f5a000.. .. ..4 pte = 0x0000000021fd641f pa = 0x0000000087f59000..255 pte = 0x0000000021fdd001 pa = 0x0000000087f74000.. ..511 pte = 0x0000000021fdcc01 pa = 0x0000000087f73000.. .. ..510 pte = 0x0000000021fd90c7 pa = 0x0000000087f64000.. .. ..511 pte = 0x0000000020001c4b pa = 0x0000000080007000第二次懒分配19号虚存页面，然后成功打印“hi”lazy allocationvmprint:..0 pte = 0x0000000021fdc801 pa = 0x0000000087f72000.. ..0 pte = 0x0000000021fd9401 pa = 0x0000000087f65000.. .. ..0 pte = 0x0000000021fdc05f pa = 0x0000000087f70000.. .. ..1 pte = 0x0000000021fd98df pa = 0x0000000087f66000.. .. ..2 pte = 0x0000000021fdc40f pa = 0x0000000087f71000.. .. ..3 pte = 0x0000000021fd68df pa = 0x0000000087f5a000.. .. ..4 pte = 0x0000000021fd64df pa = 0x0000000087f59000.. .. ..19 pte = 0x0000000021fd601f pa = 0x0000000087f58000..255 pte = 0x0000000021fdd001 pa = 0x0000000087f74000.. ..511 pte = 0x0000000021fdcc01 pa = 0x0000000087f73000.. .. ..510 pte = 0x0000000021fd90c7 pa = 0x0000000087f64000.. .. ..511 pte = 0x0000000020001c4b pa = 0x0000000080007000hiLazytests and Usertests为了保证系统功能的完好，只修改usertrap()是不够的，问题包括： uvmunmap()：解除映射的时候要求映射必须存在，但懒分配机制使得一些映射并不存在，因此需要去掉一些panic uvmcopy()：和uvmunmap()类似，需要去掉一些panic copyout()：向用户空间的指定地址写入信息，这个地址可能是懒分配的 copyin()：从用户空间的指定地址读取信息，这个地址也可能是懒分配的，虽然说这种情况意味着用户程序没往这个地址写过，有点奇怪，但确实是合法的 copyinstr()：和copyin()类似这些要改的都不多，要么是注释掉panic，要么是使用uvm_lazy_detect uvm_lazy_alloc需要注意的是page fault有很多种情况，不一定都是懒分配导致的，需要仔细甄别比如往一个只读页里写也会导致page fault，这时应该把进程杀掉，如果当作懒分配处理就会触发panic: remap" }, { "title": "【6.S081】4 Traps", "url": "/posts/6.S081-4-traps/", "categories": "", "tags": "操作系统, 6.S081", "date": "2022-02-16 00:00:04 +0000", "snippet": "这个难度还好RISC-V Assembly1、Which registers contain arguments to functions? For example, which register holds 13 in main’s call to printf?a0，a1…a5保存参数，syscall.c中的argint()负责读出通过寄存器传递的参数，可以看到它不会读a6、a713存在a2中2、Where is the call to function f in the assembly code for main? Where is the call to g? (Hint: the compiler may inline functions.)没看到main对f的调用，也没看到f对g的调用，看来是inline了后来把Makefile中CFLAGS中的-O改成-O0，然后make clean再make qemu，就能看见调用了3、At what address is the function printf located?printf在0x6304、What value is in the register ra just after the jalr to printf in main?猜ra = 0x385、问一段代码会输出什么；以及如果riscv从小端存储变成大端存储，应该怎么改才能输出一样的东西会打印HE110 World大端存储的话，i按8位颠倒过来，变成0x726c6400；57616不用改，因为大小端不影响对32位数的解读6、printf(“%d %d”, 3)，第二个数会输出什么？不确定，取决于a2寄存器当时的值backtracevoid backtrace(void) { printf(\"backtrace:\\n\"); // uint64 fp = r_fp(); uint64 fp; asm volatile(\"mv %0, s0\" : \"=r\" (fp) ); // riscv指令集让s0兼任了fp寄存器 uint64 stack = PGROUNDDOWN((uint64)fp); for (; stack==PGROUNDDOWN((uint64)fp); fp=(uint64)(*((uint64*)(fp-16)))) printf(\"%p\\n\", *(uint64*)(fp-8)); // fp-8是ra，fp-16是caller的fp}输出：$ bttestbacktrace:0x0000000080002cca0x0000000080002ba40x000000008000288eAlarm (hard)实现系统调用sigalarm(uint interval, void (*handler)())，功能为每经历interval时间，就触发一次handlertest0: invoke handler在这一问卡了很久，一直在思考如何从内核态调用用户空间的函数 直接在usertrap中对函数指针解引用是不行的，会导致缺页中断，进而kernel trap 用walkaddr(p-&gt;pagetable, p-&gt;handler)把虚拟地址转成物理地址，然后内核对物理地址解引用？gdb这个转换出来的物理地址确实正确，但内核依然访问不了 临时把satp换成p-&gt;pagetable？一换完，下一条指令就读不出来了，因为pc会被映射到错误的位置 还想出了一些涉及页表的更复杂的操作，自己都觉得不可能……这些都不靠谱，正确的解决方法是和页表没关系的先捋一遍trap处理过程|被写入|读取|实际语句|位于函数/汇编文件||:-:|:-:|:-:|:-:||$epc | $pc|ecall |usys.S|| p-&gt;trapframe-&gt;epc | $epc| p-&gt;trapframe-&gt;epc = r_sepc();|usertrap() || $epc | p-&gt;trapframe-&gt;epc | epcw_sepc(p-&gt;trapframe-&gt;epc); |usertrapret() || $pc | $epc | sret | trampoline.S|我们在usertrap()中对epc进行一个偷梁换柱，把p-&gt;handler存进p-&gt;trapframe-&gt;epc，sret返回用户态的时候$pc不就跳到handler了吗~至于怎么从handler跳回中断之前的位置，那是下一个test的事情在proc结构体中加几项，在allocproc的时候都初始化成0//proc.hstruct proc { ...... int alarm_threshold; // 计数门限 int alarm_ticks; // 当前计数值 void (*handler)(void); // 处理函数};加系统调用是lab2的内容，就不细说了添加sys_sigalarm()函数，考虑到这个函数是进程给自己加设定，放在sysproc.c中比较合适//sysproc.cint sys_sigalarm() { struct proc *p = myproc(); argint(0, &amp;(p-&gt;alarm_threshold)); int handler; argint(1, &amp;handler); p-&gt;handler = (void(*)(void))(uint64)handler; return 0;}//trap.cvoid usertrap(void){ ...... struct proc *p = myproc(); // save user program counter. p-&gt;trapframe-&gt;epc = r_sepc(); // 这里 if (devintr()==2 &amp;&amp; p-&gt;alarm_threshold&gt;0 &amp;&amp; p-&gt;alarm_in_processing==0) { p-&gt;alarm_ticks++; if (p-&gt;alarm_ticks==p-&gt;alarm_threshold) { p-&gt;trapframe-&gt;epc = (uint64)(p-&gt;handler); p-&gt;alarm_ticks = 0; } } ......}test1/test2(): resume interrupted code在proc结构体中继续加东西//proc.hstruct proc { ...... int alarm_threshold; int alarm_ticks; void (*handler)(void); int alarm_in_processing; // 一个alarm没处理完的时候，不会进入下一个alarm uint64 trapframe_backup[36]; // 提示说需要备份很多东西，那把trapframe整个备份肯定没错了};//trap.cvoid usertrap(void){ ...... struct proc *p = myproc(); // save user program counter. p-&gt;trapframe-&gt;epc = r_sepc(); // 这里 if (devintr()==2 &amp;&amp; p-&gt;alarm_threshold&gt;0 &amp;&amp; p-&gt;alarm_in_processing==0) { p-&gt;alarm_ticks++; if (p-&gt;alarm_ticks==p-&gt;alarm_threshold) { p-&gt;alarm_in_processing = 1; // 备份trapframe memmove((void*)(p-&gt;trapframe_backup), (const void*)(p-&gt;trapframe), sizeof(uint64)*36); p-&gt;trapframe-&gt;epc = (uint64)(p-&gt;handler); p-&gt;alarm_ticks = 0; } } ......}//sysproc.cint sys_sigreturn() { struct proc *p = myproc(); // 恢复备份 memmove((void*)(p-&gt;trapframe), (const void*)(p-&gt;trapframe_backup), sizeof(uint64)*36); p-&gt;alarm_in_processing = 0; p-&gt;alarm_ticks = 0; return 0;}总结== Test answers-traps.txt == answers-traps.txt: OK == Test backtrace test == $ make qemu-gdbbacktrace test: OK (2.8s) == Test running alarmtest == $ make qemu-gdb(4.2s) == Test alarmtest: test0 == alarmtest: test0: OK == Test alarmtest: test1 == alarmtest: test1: OK == Test alarmtest: test2 == alarmtest: test2: OK == Test usertests == $ make qemu-gdbusertests: OK (69.9s) == Test time == time: OK Score: 85/85首次使用gdb（考虑到跳过了lab3），太强大了" }, { "title": "【6.S081】2 System Calls", "url": "/posts/6.S081-2-system-calls/", "categories": "", "tags": "操作系统, 6.S081", "date": "2022-02-06 00:00:01 +0000", "snippet": "这次实验要写的代码不多，但要读的东西比较多通过读代码，大致可以明白系统调用的过程是 用户程序调用 “系统调用入口”，例如fork() close()，这些只是入口伪装成的函数，并不是真正的系统函数 这些入口函数的参数会被记录到a0-a6寄存器，例如close()只有一个参数，就记录到a0 在汇编层面，入口函数会被链接到user/usys.S中的标签（usys.S是由usys.pl这个perl脚本生成的） 以close入口函数为例，在usys.S中的汇编代码为 close: li a7, SYS_close // a7寄存器记录系统调用编号，SYS_close是一个宏 ecall // 产生系统调用例外，进入例外处理程序 ret cpu识别到ecall指令时，硬件上会把包括寄存器值在内的一些信息作为trapframe（具体结构见kernel/proc.h）保存到内存，然后切换到内核态，并把pc指向syscall函数，所有系统调用都经过此函数中转 syscall函数读出trapframe，按a7的值启动系统调用sys_close，这次可以认为是真正的系统调用 依次返回，cpu切换回用户态 System call tracing (moderate) 创建一个新的系统调用trace，功能是监测所有系统调用，并某些情况下将发生的调用输出到控制台 大概可以分成这么几步： 用户侧 /user/user.h 中添加函数原型 /user/usys.pl 中添加一条perl指令，用于生成trace在汇编层面的入口 内核侧 kernel/proc.h 中，结构体 proc 添加用于保存trace掩码的项 struct proc { struct spinlock lock; // p-&gt;lock must be held when using these: enum procstate state; // Process state ...... int trace_mask; // 这里 }; kernel/sys_proc.c 中，加一个函数sys_trace，它读取调用trace时的参数，然后设置本进程的trace掩码 uint64 sys_trace(void) { int mask; if (argint(0, &amp;mask) &lt; 0) return -1; myproc()-&gt;trace_mask = mask; return 0; } kernel/proc.c 中，fork函数中添加一行，将父进程的trace掩码复制给子进程 kernel/syscall.c 中，extern sys_trace，并在syscalls这个函数指针数组中追加sys_trace函数 kernel/syscall.c 中，syscall函数中添加打印trace信息的功能，而是否打印信息取决于，本进程的trace掩码和系统调用编号按位与的结果 void syscall(void) { int num; struct proc *p = myproc(); num = p-&gt;trapframe-&gt;a7; if(num &gt; 0 &amp;&amp; num &lt; NELEM(syscalls) &amp;&amp; syscalls[num]) { p-&gt;trapframe-&gt;a0 = syscalls[num](); // 这里 if ((p-&gt;trace_mask &amp; (1&lt;&lt;num)) != 0) printf(\"%d: syscall %s -&gt; %d\\n\", p-&gt;pid, syscall_name[num], p-&gt;trapframe-&gt;a0); } else { printf(\"%d %s: unknown sys call %d\\n\", p-&gt;pid, p-&gt;name, num); p-&gt;trapframe-&gt;a0 = -1; } } Sysinfo (moderate) 给出目前空闲的物理内存字节数，以及活动的进程数同样作为一个系统调用，依然是那些地方需要加东西sysinfo的两个功能分成两个函数来实现统计空闲内存 free_memory_bytes()kalloc.c负责对物理内存的分配、释放，通过阅读代码可以发现，内存管理是以页为单位的，每页4096字节所有空闲页串成一个单向链表，链表的头部就是kmem.freelist这一点搞明白之后，统计空闲内存就有两种方法： 第一种是每次统计都把链表遍历一遍 第二种是在kmem结构体中加一项链表长度，每次kalloc成功，长度就减一，每次free成功长度就+1，这种好写```cstruct { struct spinlock lock; struct run *freelist; int list_len;} kmem;int free_memory_bytes() { return kmem.list_len * 4096;}### 统计活动进程数proc.c中的proc数组包含了每个进程的状态，遍历即可```cint num_of_processes(void) { int cnt = 0; for (int i=0; i&lt;NPROC; i++) { if (proc[i].state != UNUSED) cnt++; } return cnt;}这两个函数会被被sys_sysinfo()用到，需要加到def.h里面sysinfo系统调用不知道这个函数该放哪里，索性放在sysfile.c下面了int sys_sysinfo() { // printf(\"gg!\\n\"); uint64 info; argaddr(0, &amp;info); // info其实是用户程序给出的结构体指针，这里作为int指针读出来 struct sysinfo sf; sf.freemem = free_memory_bytes(); // 调用两个自己写的函数 sf.nproc = num_of_processes(); struct proc *p = myproc(); // 暂时不明白copy的意义，不过这么用就对了 if (copyout(p-&gt;pagetable, info, (char *)&amp;sf, sizeof(sf)) &lt; 0) return -1; return 0;}" }, { "title": "【6.S081】1 Unix Utilities", "url": "/posts/6.S081-1-Util/", "categories": "", "tags": "操作系统, 6.S081", "date": "2022-02-05 00:00:00 +0000", "snippet": "本次实验中，会学习fork read write pipe等经典系统调用的用法，并实现5个Unix系统工具sleep (easy)根据实验讲义的提示，参考user/user.h中sleep系统调用的格式，并参考echo.c的写法课程也提供了测试方法./grade-lab-util sleep我的测试结果为make: 'kernel/kernel' is up to date.== Test sleep, no arguments == sleep, no arguments: OK (0.9s) == Test sleep, returns == sleep, returns: OK (1.2s) == Test sleep, makes syscall == sleep, makes syscall: OK (1.0s) 也尝试了一下故意写错，结果是这样的make: 'kernel/kernel' is up to date.== Test sleep, no arguments == sleep, no arguments: OK (1.1s) == Test sleep, returns == sleep, returns: OK (1.3s) == Test sleep, makes syscall == sleep, makes syscall: FAIL (0.9s) ... hart 1 starting init: starting sh GOOD $ sleep 10 $ echo FAIL BAD FAIL $ qemu-system-riscv64: terminating on signal 15 from pid 5776 (make) unexpected lines in output QEMU output saved to xv6.out.sleeppingpong(easy)视频没讲pipe()的用法，得参考xv6 book：https://pdos.csail.mit.edu/6.828/2020/xv6/book-riscv-rev1.pdfpipe的创建是 int p[2]; pipe(p);之后再fork，两个进程都使用p[0]接收对方的信息，p[1]发送信息find (moderate)主要参考user/ls.c，改一点就可以了，递归地处理文件目录ls.c比较关键的是这两个结构体，和fstat这个系统调用struct dirent { ushort inum; // 大概是编号 char name[DIRSIZ]; // 文件名};#define T_DIR 1 // Directory#define T_FILE 2 // File#define T_DEVICE 3 // Devicestruct stat { int dev; // File system's disk device uint ino; // Inode number short type; // Type of file short nlink; // Number of links to file uint64 size; // Size of file in bytes};int fstat(int fd, struct stat*); // 读取文件描述符fd，将文件的属性写入statread可以连续读取文件夹，每次读出一个文件的名字，并填充dirent结构体read(fd, &amp;de, sizeof(de))一个坑点是read出来的结果中包括了.和..，不甄别这两种情况就会导致无限递归$ find . ok../.././../././.././././../././././.usertrap(): unexpected scause 0x000000000000000f pid=3 sepc=0x0000000000000886 stval=0x0000000000001ea8正常结果是$ find . zombie./zombie./zz/zombie$ mkdir zz/z$ echo gg &gt; zz/z/zombie$ find . zombie./zombie./zz/zombie./zz/z/zombiexargs先看看xv6的shell对管道命令(|)的实现 case PIPE: pcmd = (struct pipecmd*)cmd; if(pipe(p) &lt; 0) panic(\"pipe\"); if(fork1() == 0){ close(1); // 关掉1号文件描述符，1原本代表标准输出 dup(p[1]); // 找出空闲的值最小的文件描述符（也就是1），让它和p[1]指向同一个文件 close(p[0]); close(p[1]); runcmd(pcmd-&gt;left); // pcmd-&gt;left这条指令如果有输出，会直接输出到1号文件，不需要管1号到底是什么 } if(fork1() == 0){ close(0); dup(p[0]); close(p[0]); close(p[1]); runcmd(pcmd-&gt;right); // 同理，pcmd-&gt;right现在会从0号文件读取信息，这些信息实际来自pcmd-&gt;right } close(p[0]); close(p[1]); wait(0); wait(0); break;runcmd最终会执行exec()，这个函数是不会返回的，因此fork1产生的子进程不会返回，第二次fork1只有父进程才执行那么管道指令的原理就可以概括为：父进程创建两个子进程，其中子进程1运行管道符左侧的指令，子进程2运行管道符右边的指令，子进程1向子进程2通过管道发送信息另外就是注意exec函数的格式这些搞清楚之后，xargs本身的功能很好实现，xargs这条指令就相当于pcmd-&gt;right嘛，从0号文件读，读到的东西按’\\n’分割一下，分别追加到参数列表的后面就行如果能用strtok，这波将绝杀，可惜用不得，include标准库头文件会报错，因为和xv6中的实现有冲突primes (hard)并发素数筛是由复数个进程串联形成的，类似于流水线第0级把2-35这些数发送到第1级第1-n级的工作方式完全相同，都是把接收到的第一个数当成素数，然后试除接下来收到的所有数，如果除不尽就发送到下一级原理搞清楚之后就比较好实现，记得把不需要的管道关掉" }, { "title": "【6.S081】0 MIT操作系统课程介绍与实验环境配置", "url": "/posts/6.S081-0-%E8%AF%BE%E7%A8%8B%E4%BB%8B%E7%BB%8D%E4%B8%8E%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-copy/", "categories": "", "tags": "操作系统, 6.S081", "date": "2022-02-04 00:00:01 +0000", "snippet": "6.S081是mit为本科生开设的AUS（Advanced Undergraduate Subjects，本科生进阶课程），之前经典的6.828似乎拆成了6.S081和6.S082两门，看来mit的学生也肝不动6.828每个Lab难度参差不齐，简单的几小时搞定，难的能花好几天.每个Lab都有完善的离线评测，十分良心, 值得一刷.感想这部分是完成全部实验之后补充的每个Lab的难度1 Xv6 and Unix utilities: 码量最大, 也是最简单的一个lab2 system calls: 系统调用, 需要读的东西比较多, 难度适中3 page tables: 需要全面理解xv6虚存机制, 并大幅修改, 噩梦难度4 traps: 异常, 难度适中, 比较有趣的一个lab5 lazy page allocation: 内存懒分配, 需要部分理解虚存, 难度适中6 Copy-on-Write Fork: 写时复制的fork, 需要全面理解虚存, 难度高7 Multithreading: 多线程, 需要了解一点xv6线程切换, 难度适中8 locks: 锁, 坑不少, 较难, 但有趣9 file system: 码量小, 不熟悉文件系统的话就有难了10 mmap: 文件内存映射, 结合了虚存与文件系统, 码量略大, 难度仅次于lab311 networking: 码量小, 要读一些东西总结 受锻炼最大的应该是debug的勇气. 不管遇到多少个bug, 都不要怕, 微笑着面对它 大学里肯定也会开操作系统, 建议上完那个再来肝这个, 从二者的区别能收获不少东西 前两个实验比较简单, 而lab3 pgtbl堪称11个实验中最难的, 当时我硬着头皮研究了4天也没搞定, 差点给劝退了, 后来缓了一周, 跳过lab3继续往后做, 做完lab6之后回过头来又花了2天啃下lab3. 写总结的时候, 去看了一下2021年的安排, 基本和20年一样, 唯独lab3改简单了, 确实更合理一些, 不过本系列还是按照20年的实验来. 再说了就算你做21年的实验, 难道就不想挑战一下吗. gdb是非常强大的工具, qemu对gdb有非常完善的支持, 不品尝就太可惜了. gdb的用处不限于: layout split进行C语言-汇编联合调试 用where指令查看函数调用栈, 查看是什么过程触发了panic 用print指令查看任意局部变量和寄存器的值, 甚至*p这样的表达式也是可行的 用thread指令查看不同cpu核心的运行情况 实验一般会提供很细致的hint, 完全依赖hint一般也能做出来, 不过如果自己能先琢磨一下怎么实现会更好 过不了测试却没有什么思路的时候, 可以考虑直接去看测试程序, 有些细节在题面中没有提到. xv6真是非常注重变量命名的统一. 虽然看题和看书可以靠谷歌翻译, 但咱通过啃英文的过程提升了一点阅读能力…课程资源 课程视频：https://www.bilibili.com/video/BV19k4y1C7kA 课程目录：https://pdos.csail.mit.edu/6.828/2020/schedule.html 课程讲义（中文）：https://mit-public-courses-cn-translatio.gitbook.io/mit6-s081/ xv6操作系统手册（英文）：https://pdos.csail.mit.edu/6.828/2020/xv6/book-riscv-rev1.pdf环境配置vmware虚拟机安装ubuntu 20.04.3，建议分配40GB磁盘空间，默认的20GB是不够的，riscv工具链解压就需要10GB空间。环境配置主要看这里：https://pdos.csail.mit.edu/6.828/2020/tools.html我这里就是讲一些坑点riscv工具链如果你的网络很好，可以按官网的说法git clonegit clone --recursive https://github.com/riscv/riscv-gnu-toolchain但是如果网络不好的话，比如你在国内，基本速度就只有10KB/s，即使配置了代理在下载一部分文件的时候依然很慢，而文件总共有3.5GB哦 XD还好可以从百度网盘下载，然后用U盘拷贝到虚拟机中https://pan.baidu.com/s/1104aCZiIUAEcHONKcI8GKw 提取码：ui4j安装编译工具链所需要的东西sudo apt-get install autoconf automake autotools-dev curl libmpc-dev libmpfr-dev libgmp-dev gawk build-essential bison flex texinfo gperf libtool patchutils bc zlib1g-dev libexpat-devcd到工具链目录下，执行两条命令./configure --prefix=/usr/local/opt/riscv-gnu-toolchainmake编译工具链需要半小时，这段时间可以准备一下qemuqemu按官网所说，下载qemu 5.1.0并解压缩wget https://download.qemu.org/qemu-5.1.0.tar.xz tar xf qemu-5.1.0.tar.xz执行一系列指令cd qemu-5.1.0 ./configure --disable-kvm --disable-werror --prefix=/usr/local --target-list=\"riscv64-softmmu\" make sudo make install cd ..其中最长的那行命令可能出问题，如果报这个错误:ERROR: glib-2.48 gthread-2.0 is required to compile QEMU解决方法为sudo apt install libglib2.0-dev还可能报这个错误：ERROR: pixman &gt;= 0.21.8 not present. Please install the pixman devel package.解决方法为sudo apt install libpixman-1-dev一切正常的话, 输出的最后是default devices yesplugin support nofuzzing support nogdb /usr/bin/gdbrng-none noLinux keyring yescross containers noxv6操作系统源码直接clone吧，虽然速度慢，好在文件不大git clone git://g.csail.mit.edu/xv6-labs-2020你可能会发现clone下来的这个文件夹是空的，别担心，git就是这样的，运行这两条指令cd xv6-labs-2020git checkout util文件夹里就有东西了，很神奇吧gdb前两个lab用不着gdb，可以等做第三个lab的时候再来配置配置安装完工具链之后，gdb就已经安装好了用户目录（~）下创建一个名字叫.gdbinit的文件，内容是add-auto-load-safe-path ~/mit6.s/xv6-labs-2020/.gdbinit 后半句改成你自己的 xv6-labs-2020 文件夹所在位置cd到 xv6-labs-2020/ 下，运行命令make CPUS=1 qemu-gdb注意CPUS=1不能省略，让qemu模拟单处理器然后另外开一个terminal，也cd到 xv6-labs-2020/ 下，运行命令riscv64-unknown-elf-gdb在main函数打个断点，然后continue0x0000000000001000 in ?? ()(gdb) break mainBreakpoint 1 at 0x80000ec6: file kernel/main.c, line 13.(gdb) continueContinuing.Breakpoint 1, main () at kernel/main.c:1313\t if(cpuid() == 0){(gdb) 这样就是正常的更好用的gdb从lecture4开始，老师会展示gdb的一个神奇功能，输入layout split，三栏分别显示c代码、汇编代码、控制台 这个功能太好用了，但是按上述流程安装的riscv64-unknown-elf-gdb并不支持layout系列命令所以这里隆重推荐：gdb-multiarch，安装只需一行sudo apt install gdb-multiarch运行方法和riscv64-unknown-elf-gdb完全一致，而且不需要配置其它东西，就可以使用layout命令vscode remote一开始我是在ubuntu虚拟机下写代码，虽然ubuntu的图形界面也搞得不错，但是毕竟没有win10流畅，拖动窗口的时候卡卡的，用chrome查资料有时候还卡住，而且我在win做的一些快捷键没法在ubuntu中实现后来就发现了vscode远程连接到本地虚拟机这个法子，只能说牛逼因为有很多东西在linux下好弄，甚至只能在linux下弄，但是我又喜欢win的桌面体验，这下两全齐美了具体怎么配网上讲的很多，我已经忘了最后上一张调试Lab8的效果图 " }, { "title": "try", "url": "/posts/try/", "categories": "TOP_CATEGORIE, SUB_CATEGORIE", "tags": "", "date": "2000-01-01 00:00:00 +0000", "snippet": "测试" } ]
